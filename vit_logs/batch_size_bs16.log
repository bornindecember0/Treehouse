Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.0001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 16,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.05,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "standard",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "use_cls_token": true,
  "global_pool": "cls",
  "exp_name": "batch_size_bs16",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.8357, train_acc=0.0057, val_loss=5.8313, val_acc=0.0040, lr=0.0000
Epoch 2: train_loss=5.8245, train_acc=0.0048, val_loss=5.8204, val_acc=0.0041, lr=0.0000
Epoch 3: train_loss=5.8136, train_acc=0.0047, val_loss=5.7992, val_acc=0.0038, lr=0.0000
Epoch 4: train_loss=5.7995, train_acc=0.0047, val_loss=5.7696, val_acc=0.0040, lr=0.0000
Epoch 5: train_loss=5.7649, train_acc=0.0045, val_loss=5.7372, val_acc=0.0041, lr=0.0000
Epoch 6: train_loss=5.7429, train_acc=0.0040, val_loss=5.7000, val_acc=0.0036, lr=0.0000
Epoch 7: train_loss=5.6960, train_acc=0.0052, val_loss=5.6615, val_acc=0.0038, lr=0.0000
Epoch 8: train_loss=5.6629, train_acc=0.0043, val_loss=5.6240, val_acc=0.0043, lr=0.0000
Epoch 9: train_loss=5.6124, train_acc=0.0043, val_loss=5.5879, val_acc=0.0041, lr=0.0000
Epoch 10: train_loss=5.5857, train_acc=0.0075, val_loss=5.5537, val_acc=0.0047, lr=0.0000
Epoch 11: train_loss=5.5377, train_acc=0.0052, val_loss=5.5217, val_acc=0.0055, lr=0.0000
Epoch 12: train_loss=5.5115, train_acc=0.0042, val_loss=5.4926, val_acc=0.0052, lr=0.0000
Epoch 13: train_loss=5.4961, train_acc=0.0067, val_loss=5.4653, val_acc=0.0059, lr=0.0000
Epoch 14: train_loss=5.4639, train_acc=0.0057, val_loss=5.4390, val_acc=0.0060, lr=0.0000
Epoch 15: train_loss=5.4370, train_acc=0.0072, val_loss=5.4143, val_acc=0.0047, lr=0.0000
Epoch 16: train_loss=5.4029, train_acc=0.0085, val_loss=5.3925, val_acc=0.0064, lr=0.0000
Epoch 17: train_loss=5.3864, train_acc=0.0087, val_loss=5.3734, val_acc=0.0067, lr=0.0000
Epoch 18: train_loss=5.3625, train_acc=0.0095, val_loss=5.3557, val_acc=0.0072, lr=0.0000
Epoch 19: train_loss=5.3451, train_acc=0.0098, val_loss=5.3386, val_acc=0.0086, lr=0.0000
Epoch 20: train_loss=5.3350, train_acc=0.0100, val_loss=5.3226, val_acc=0.0093, lr=0.0000
Epoch 21: train_loss=5.3173, train_acc=0.0087, val_loss=5.3073, val_acc=0.0102, lr=0.0000
Epoch 22: train_loss=5.2964, train_acc=0.0103, val_loss=5.2932, val_acc=0.0119, lr=0.0000
Epoch 23: train_loss=5.2756, train_acc=0.0110, val_loss=5.2806, val_acc=0.0114, lr=0.0000
Epoch 24: train_loss=5.2684, train_acc=0.0120, val_loss=5.2671, val_acc=0.0109, lr=0.0000
Epoch 25: train_loss=5.2500, train_acc=0.0128, val_loss=5.2555, val_acc=0.0117, lr=0.0000
Epoch 26: train_loss=5.2385, train_acc=0.0105, val_loss=5.2451, val_acc=0.0131, lr=0.0000
Epoch 27: train_loss=5.2330, train_acc=0.0133, val_loss=5.2347, val_acc=0.0133, lr=0.0000
Epoch 28: train_loss=5.2144, train_acc=0.0130, val_loss=5.2230, val_acc=0.0155, lr=0.0000
Epoch 29: train_loss=5.2073, train_acc=0.0123, val_loss=5.2145, val_acc=0.0148, lr=0.0000
Epoch 30: train_loss=5.1914, train_acc=0.0145, val_loss=5.2035, val_acc=0.0159, lr=0.0000
Epoch 31: train_loss=5.1802, train_acc=0.0160, val_loss=5.1961, val_acc=0.0157, lr=0.0000
Epoch 32: train_loss=5.1676, train_acc=0.0142, val_loss=5.1868, val_acc=0.0161, lr=0.0000
Epoch 33: train_loss=5.1577, train_acc=0.0162, val_loss=5.1804, val_acc=0.0183, lr=0.0000
Epoch 34: train_loss=5.1509, train_acc=0.0172, val_loss=5.1723, val_acc=0.0164, lr=0.0000
Epoch 35: train_loss=5.1286, train_acc=0.0182, val_loss=5.1635, val_acc=0.0193, lr=0.0000
Epoch 36: train_loss=5.1229, train_acc=0.0195, val_loss=5.1580, val_acc=0.0195, lr=0.0000
Epoch 37: train_loss=5.1210, train_acc=0.0175, val_loss=5.1533, val_acc=0.0181, lr=0.0000
Epoch 38: train_loss=5.1127, train_acc=0.0202, val_loss=5.1444, val_acc=0.0197, lr=0.0000
Epoch 39: train_loss=5.0994, train_acc=0.0163, val_loss=5.1400, val_acc=0.0200, lr=0.0000
Epoch 40: train_loss=5.0888, train_acc=0.0215, val_loss=5.1323, val_acc=0.0195, lr=0.0000
Epoch 41: train_loss=5.0842, train_acc=0.0204, val_loss=5.1255, val_acc=0.0209, lr=0.0000
Epoch 42: train_loss=5.0763, train_acc=0.0202, val_loss=5.1208, val_acc=0.0204, lr=0.0000
Epoch 43: train_loss=5.0565, train_acc=0.0252, val_loss=5.1176, val_acc=0.0207, lr=0.0000
Epoch 44: train_loss=5.0582, train_acc=0.0212, val_loss=5.1127, val_acc=0.0204, lr=0.0000
Epoch 45: train_loss=5.0631, train_acc=0.0207, val_loss=5.1048, val_acc=0.0214, lr=0.0000
Epoch 46: train_loss=5.0395, train_acc=0.0282, val_loss=5.0967, val_acc=0.0200, lr=0.0000
Epoch 47: train_loss=5.0277, train_acc=0.0235, val_loss=5.0958, val_acc=0.0200, lr=0.0000
Epoch 48: train_loss=5.0223, train_acc=0.0239, val_loss=5.0878, val_acc=0.0223, lr=0.0000
Epoch 49: train_loss=5.0168, train_acc=0.0269, val_loss=5.0836, val_acc=0.0223, lr=0.0000
Epoch 50: train_loss=5.0057, train_acc=0.0237, val_loss=5.0767, val_acc=0.0226, lr=0.0000
Epoch 51: train_loss=4.9956, train_acc=0.0270, val_loss=5.0727, val_acc=0.0219, lr=0.0000
Epoch 52: train_loss=4.9905, train_acc=0.0270, val_loss=5.0658, val_acc=0.0235, lr=0.0000
Epoch 53: train_loss=4.9811, train_acc=0.0265, val_loss=5.0619, val_acc=0.0230, lr=0.0000
Epoch 54: train_loss=4.9717, train_acc=0.0285, val_loss=5.0558, val_acc=0.0233, lr=0.0000
Epoch 55: train_loss=4.9665, train_acc=0.0267, val_loss=5.0488, val_acc=0.0243, lr=0.0000
Epoch 56: train_loss=4.9555, train_acc=0.0325, val_loss=5.0450, val_acc=0.0226, lr=0.0000
Epoch 57: train_loss=4.9410, train_acc=0.0330, val_loss=5.0368, val_acc=0.0254, lr=0.0000
Epoch 58: train_loss=4.9274, train_acc=0.0334, val_loss=5.0287, val_acc=0.0262, lr=0.0000
Epoch 59: train_loss=4.9200, train_acc=0.0320, val_loss=5.0262, val_acc=0.0264, lr=0.0000
Epoch 60: train_loss=4.9064, train_acc=0.0339, val_loss=5.0218, val_acc=0.0288, lr=0.0000
Epoch 61: train_loss=4.8959, train_acc=0.0365, val_loss=5.0155, val_acc=0.0288, lr=0.0000
Epoch 62: train_loss=4.8837, train_acc=0.0315, val_loss=5.0077, val_acc=0.0312, lr=0.0000
Epoch 63: train_loss=4.8824, train_acc=0.0354, val_loss=4.9976, val_acc=0.0283, lr=0.0000
Epoch 64: train_loss=4.8634, train_acc=0.0384, val_loss=4.9937, val_acc=0.0305, lr=0.0000
Epoch 65: train_loss=4.8557, train_acc=0.0402, val_loss=4.9871, val_acc=0.0312, lr=0.0000
Epoch 66: train_loss=4.8434, train_acc=0.0397, val_loss=4.9796, val_acc=0.0321, lr=0.0000
Epoch 67: train_loss=4.8303, train_acc=0.0394, val_loss=4.9722, val_acc=0.0328, lr=0.0000
Epoch 68: train_loss=4.8231, train_acc=0.0384, val_loss=4.9653, val_acc=0.0357, lr=0.0000
Epoch 69: train_loss=4.8034, train_acc=0.0450, val_loss=4.9599, val_acc=0.0350, lr=0.0000
Epoch 70: train_loss=4.7922, train_acc=0.0427, val_loss=4.9551, val_acc=0.0364, lr=0.0000
Epoch 71: train_loss=4.7763, train_acc=0.0457, val_loss=4.9515, val_acc=0.0352, lr=0.0000
Epoch 72: train_loss=4.7693, train_acc=0.0472, val_loss=4.9396, val_acc=0.0359, lr=0.0000
Epoch 73: train_loss=4.7617, train_acc=0.0419, val_loss=4.9358, val_acc=0.0371, lr=0.0000
Epoch 74: train_loss=4.7459, train_acc=0.0469, val_loss=4.9322, val_acc=0.0366, lr=0.0000
Epoch 75: train_loss=4.7410, train_acc=0.0452, val_loss=4.9221, val_acc=0.0373, lr=0.0000
Epoch 76: train_loss=4.7185, train_acc=0.0519, val_loss=4.9121, val_acc=0.0390, lr=0.0000
Epoch 77: train_loss=4.7074, train_acc=0.0497, val_loss=4.9064, val_acc=0.0385, lr=0.0000
Epoch 78: train_loss=4.7009, train_acc=0.0497, val_loss=4.8955, val_acc=0.0394, lr=0.0000
Epoch 79: train_loss=4.6908, train_acc=0.0470, val_loss=4.8866, val_acc=0.0404, lr=0.0000
Epoch 80: train_loss=4.6744, train_acc=0.0554, val_loss=4.8750, val_acc=0.0416, lr=0.0000
Epoch 81: train_loss=4.6643, train_acc=0.0516, val_loss=4.8708, val_acc=0.0411, lr=0.0000
Epoch 82: train_loss=4.6434, train_acc=0.0592, val_loss=4.8634, val_acc=0.0421, lr=0.0000
Epoch 83: train_loss=4.6403, train_acc=0.0604, val_loss=4.8537, val_acc=0.0426, lr=0.0000
Epoch 84: train_loss=4.6278, train_acc=0.0571, val_loss=4.8453, val_acc=0.0440, lr=0.0000
Epoch 85: train_loss=4.6072, train_acc=0.0589, val_loss=4.8350, val_acc=0.0445, lr=0.0000
Epoch 86: train_loss=4.5984, train_acc=0.0611, val_loss=4.8287, val_acc=0.0433, lr=0.0000
Epoch 87: train_loss=4.5875, train_acc=0.0632, val_loss=4.8238, val_acc=0.0463, lr=0.0000
Epoch 88: train_loss=4.5632, train_acc=0.0672, val_loss=4.8156, val_acc=0.0463, lr=0.0000
Epoch 89: train_loss=4.5452, train_acc=0.0641, val_loss=4.8023, val_acc=0.0469, lr=0.0000
Epoch 90: train_loss=4.5393, train_acc=0.0664, val_loss=4.7929, val_acc=0.0456, lr=0.0000
Epoch 91: train_loss=4.5312, train_acc=0.0667, val_loss=4.7897, val_acc=0.0485, lr=0.0000
Epoch 92: train_loss=4.5177, train_acc=0.0709, val_loss=4.7766, val_acc=0.0483, lr=0.0000
Epoch 93: train_loss=4.4988, train_acc=0.0737, val_loss=4.7730, val_acc=0.0475, lr=0.0000
Epoch 94: train_loss=4.4781, train_acc=0.0741, val_loss=4.7613, val_acc=0.0487, lr=0.0000
Epoch 95: train_loss=4.4669, train_acc=0.0744, val_loss=4.7595, val_acc=0.0494, lr=0.0000
Epoch 96: train_loss=4.4643, train_acc=0.0731, val_loss=4.7526, val_acc=0.0485, lr=0.0000
Epoch 97: train_loss=4.4430, train_acc=0.0797, val_loss=4.7400, val_acc=0.0507, lr=0.0000
Epoch 98: train_loss=4.4196, train_acc=0.0849, val_loss=4.7376, val_acc=0.0526, lr=0.0000
Epoch 99: train_loss=4.4136, train_acc=0.0864, val_loss=4.7356, val_acc=0.0511, lr=0.0000
Epoch 100: train_loss=4.3940, train_acc=0.0879, val_loss=4.7267, val_acc=0.0497, lr=0.0000
Training completed. Best validation accuracy: 0.0526
