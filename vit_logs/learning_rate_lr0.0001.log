Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.0001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 32,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.05,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "standard",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "use_cls_token": true,
  "global_pool": "cls",
  "exp_name": "learning_rate_lr0.0001",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.8568, train_acc=0.0028, val_loss=5.8611, val_acc=0.0041, lr=0.0000
Epoch 2: train_loss=5.8552, train_acc=0.0032, val_loss=5.8477, val_acc=0.0045, lr=0.0000
Epoch 3: train_loss=5.8329, train_acc=0.0050, val_loss=5.8239, val_acc=0.0045, lr=0.0000
Epoch 4: train_loss=5.7990, train_acc=0.0042, val_loss=5.7907, val_acc=0.0045, lr=0.0000
Epoch 5: train_loss=5.7846, train_acc=0.0038, val_loss=5.7464, val_acc=0.0055, lr=0.0000
Epoch 6: train_loss=5.7321, train_acc=0.0047, val_loss=5.6939, val_acc=0.0048, lr=0.0000
Epoch 7: train_loss=5.6932, train_acc=0.0052, val_loss=5.6424, val_acc=0.0047, lr=0.0000
Epoch 8: train_loss=5.6285, train_acc=0.0055, val_loss=5.5876, val_acc=0.0047, lr=0.0000
Epoch 9: train_loss=5.5878, train_acc=0.0073, val_loss=5.5349, val_acc=0.0048, lr=0.0000
Epoch 10: train_loss=5.5307, train_acc=0.0042, val_loss=5.4905, val_acc=0.0054, lr=0.0000
Epoch 11: train_loss=5.4923, train_acc=0.0068, val_loss=5.4517, val_acc=0.0064, lr=0.0000
Epoch 12: train_loss=5.4608, train_acc=0.0063, val_loss=5.4188, val_acc=0.0071, lr=0.0000
Epoch 13: train_loss=5.4285, train_acc=0.0062, val_loss=5.3919, val_acc=0.0072, lr=0.0000
Epoch 14: train_loss=5.4111, train_acc=0.0080, val_loss=5.3671, val_acc=0.0074, lr=0.0000
Epoch 15: train_loss=5.3778, train_acc=0.0062, val_loss=5.3453, val_acc=0.0085, lr=0.0000
Epoch 16: train_loss=5.3482, train_acc=0.0102, val_loss=5.3275, val_acc=0.0079, lr=0.0000
Epoch 17: train_loss=5.3410, train_acc=0.0087, val_loss=5.3117, val_acc=0.0088, lr=0.0000
Epoch 18: train_loss=5.3200, train_acc=0.0105, val_loss=5.2979, val_acc=0.0095, lr=0.0000
Epoch 19: train_loss=5.3063, train_acc=0.0098, val_loss=5.2822, val_acc=0.0085, lr=0.0000
Epoch 20: train_loss=5.2773, train_acc=0.0103, val_loss=5.2688, val_acc=0.0093, lr=0.0000
Epoch 21: train_loss=5.2610, train_acc=0.0112, val_loss=5.2546, val_acc=0.0109, lr=0.0000
Epoch 22: train_loss=5.2488, train_acc=0.0108, val_loss=5.2423, val_acc=0.0116, lr=0.0000
Epoch 23: train_loss=5.2324, train_acc=0.0127, val_loss=5.2329, val_acc=0.0124, lr=0.0000
Epoch 24: train_loss=5.2189, train_acc=0.0140, val_loss=5.2214, val_acc=0.0131, lr=0.0000
Epoch 25: train_loss=5.2168, train_acc=0.0148, val_loss=5.2115, val_acc=0.0135, lr=0.0000
Epoch 26: train_loss=5.2014, train_acc=0.0125, val_loss=5.2003, val_acc=0.0133, lr=0.0000
Epoch 27: train_loss=5.1881, train_acc=0.0142, val_loss=5.1917, val_acc=0.0133, lr=0.0000
Epoch 28: train_loss=5.1768, train_acc=0.0180, val_loss=5.1825, val_acc=0.0135, lr=0.0000
Epoch 29: train_loss=5.1522, train_acc=0.0145, val_loss=5.1757, val_acc=0.0162, lr=0.0000
Epoch 30: train_loss=5.1463, train_acc=0.0148, val_loss=5.1656, val_acc=0.0155, lr=0.0000
Epoch 31: train_loss=5.1435, train_acc=0.0145, val_loss=5.1571, val_acc=0.0166, lr=0.0000
Epoch 32: train_loss=5.1240, train_acc=0.0190, val_loss=5.1494, val_acc=0.0192, lr=0.0000
Epoch 33: train_loss=5.1151, train_acc=0.0190, val_loss=5.1416, val_acc=0.0195, lr=0.0000
Epoch 34: train_loss=5.1079, train_acc=0.0180, val_loss=5.1325, val_acc=0.0200, lr=0.0000
Epoch 35: train_loss=5.0968, train_acc=0.0177, val_loss=5.1263, val_acc=0.0209, lr=0.0000
Epoch 36: train_loss=5.0742, train_acc=0.0194, val_loss=5.1188, val_acc=0.0204, lr=0.0000
Epoch 37: train_loss=5.0695, train_acc=0.0182, val_loss=5.1111, val_acc=0.0204, lr=0.0000
Epoch 38: train_loss=5.0515, train_acc=0.0217, val_loss=5.1047, val_acc=0.0219, lr=0.0000
Epoch 39: train_loss=5.0470, train_acc=0.0232, val_loss=5.0965, val_acc=0.0216, lr=0.0000
Epoch 40: train_loss=5.0416, train_acc=0.0237, val_loss=5.0896, val_acc=0.0230, lr=0.0000
Epoch 41: train_loss=5.0328, train_acc=0.0219, val_loss=5.0802, val_acc=0.0224, lr=0.0000
Epoch 42: train_loss=5.0237, train_acc=0.0270, val_loss=5.0796, val_acc=0.0226, lr=0.0000
Epoch 43: train_loss=5.0119, train_acc=0.0262, val_loss=5.0667, val_acc=0.0243, lr=0.0000
Epoch 44: train_loss=5.0038, train_acc=0.0260, val_loss=5.0643, val_acc=0.0228, lr=0.0000
Epoch 45: train_loss=4.9829, train_acc=0.0284, val_loss=5.0565, val_acc=0.0247, lr=0.0000
Epoch 46: train_loss=4.9793, train_acc=0.0294, val_loss=5.0494, val_acc=0.0247, lr=0.0000
Epoch 47: train_loss=4.9676, train_acc=0.0289, val_loss=5.0416, val_acc=0.0268, lr=0.0000
Epoch 48: train_loss=4.9568, train_acc=0.0290, val_loss=5.0318, val_acc=0.0249, lr=0.0000
Epoch 49: train_loss=4.9378, train_acc=0.0330, val_loss=5.0276, val_acc=0.0285, lr=0.0000
Epoch 50: train_loss=4.9303, train_acc=0.0275, val_loss=5.0203, val_acc=0.0278, lr=0.0000
Epoch 51: train_loss=4.9144, train_acc=0.0305, val_loss=5.0138, val_acc=0.0259, lr=0.0000
Epoch 52: train_loss=4.9104, train_acc=0.0347, val_loss=5.0058, val_acc=0.0300, lr=0.0000
Epoch 53: train_loss=4.8884, train_acc=0.0364, val_loss=4.9946, val_acc=0.0305, lr=0.0000
Epoch 54: train_loss=4.8811, train_acc=0.0342, val_loss=4.9913, val_acc=0.0309, lr=0.0000
Epoch 55: train_loss=4.8704, train_acc=0.0384, val_loss=4.9796, val_acc=0.0305, lr=0.0000
Epoch 56: train_loss=4.8538, train_acc=0.0387, val_loss=4.9695, val_acc=0.0319, lr=0.0000
Epoch 57: train_loss=4.8437, train_acc=0.0350, val_loss=4.9609, val_acc=0.0312, lr=0.0000
Epoch 58: train_loss=4.8291, train_acc=0.0394, val_loss=4.9467, val_acc=0.0318, lr=0.0000
Epoch 59: train_loss=4.8121, train_acc=0.0437, val_loss=4.9395, val_acc=0.0333, lr=0.0000
Epoch 60: train_loss=4.7971, train_acc=0.0419, val_loss=4.9300, val_acc=0.0354, lr=0.0000
Epoch 61: train_loss=4.7839, train_acc=0.0452, val_loss=4.9228, val_acc=0.0361, lr=0.0000
Epoch 62: train_loss=4.7704, train_acc=0.0479, val_loss=4.9100, val_acc=0.0354, lr=0.0000
Epoch 63: train_loss=4.7518, train_acc=0.0464, val_loss=4.9061, val_acc=0.0381, lr=0.0000
Epoch 64: train_loss=4.7295, train_acc=0.0502, val_loss=4.8932, val_acc=0.0364, lr=0.0000
Epoch 65: train_loss=4.7200, train_acc=0.0541, val_loss=4.8881, val_acc=0.0366, lr=0.0000
Epoch 66: train_loss=4.7099, train_acc=0.0554, val_loss=4.8735, val_acc=0.0371, lr=0.0000
Epoch 67: train_loss=4.6905, train_acc=0.0511, val_loss=4.8656, val_acc=0.0383, lr=0.0000
Epoch 68: train_loss=4.6746, train_acc=0.0574, val_loss=4.8484, val_acc=0.0378, lr=0.0000
Epoch 69: train_loss=4.6483, train_acc=0.0577, val_loss=4.8474, val_acc=0.0412, lr=0.0000
Epoch 70: train_loss=4.6399, train_acc=0.0601, val_loss=4.8256, val_acc=0.0409, lr=0.0000
Epoch 71: train_loss=4.6184, train_acc=0.0632, val_loss=4.8175, val_acc=0.0414, lr=0.0000
Epoch 72: train_loss=4.5959, train_acc=0.0641, val_loss=4.8072, val_acc=0.0426, lr=0.0000
Epoch 73: train_loss=4.5824, train_acc=0.0654, val_loss=4.7933, val_acc=0.0430, lr=0.0000
Epoch 74: train_loss=4.5612, train_acc=0.0652, val_loss=4.7849, val_acc=0.0457, lr=0.0000
Epoch 75: train_loss=4.5531, train_acc=0.0692, val_loss=4.7786, val_acc=0.0476, lr=0.0000
Epoch 76: train_loss=4.5348, train_acc=0.0694, val_loss=4.7627, val_acc=0.0445, lr=0.0000
Epoch 77: train_loss=4.5211, train_acc=0.0739, val_loss=4.7478, val_acc=0.0469, lr=0.0000
Epoch 78: train_loss=4.4927, train_acc=0.0711, val_loss=4.7345, val_acc=0.0502, lr=0.0000
Epoch 79: train_loss=4.4770, train_acc=0.0729, val_loss=4.7290, val_acc=0.0504, lr=0.0000
Epoch 80: train_loss=4.4514, train_acc=0.0791, val_loss=4.7217, val_acc=0.0495, lr=0.0000
Epoch 81: train_loss=4.4285, train_acc=0.0844, val_loss=4.7137, val_acc=0.0514, lr=0.0000
Epoch 82: train_loss=4.4198, train_acc=0.0821, val_loss=4.7007, val_acc=0.0535, lr=0.0000
Epoch 83: train_loss=4.4046, train_acc=0.0816, val_loss=4.6904, val_acc=0.0528, lr=0.0000
Epoch 84: train_loss=4.3780, train_acc=0.0844, val_loss=4.6795, val_acc=0.0559, lr=0.0000
Epoch 85: train_loss=4.3693, train_acc=0.0886, val_loss=4.6651, val_acc=0.0561, lr=0.0000
Epoch 86: train_loss=4.3469, train_acc=0.0896, val_loss=4.6714, val_acc=0.0544, lr=0.0000
Epoch 87: train_loss=4.3356, train_acc=0.0904, val_loss=4.6551, val_acc=0.0595, lr=0.0000
Epoch 88: train_loss=4.3231, train_acc=0.0923, val_loss=4.6541, val_acc=0.0592, lr=0.0000
Epoch 89: train_loss=4.2910, train_acc=0.0989, val_loss=4.6414, val_acc=0.0590, lr=0.0000
Epoch 90: train_loss=4.2917, train_acc=0.1008, val_loss=4.6302, val_acc=0.0613, lr=0.0000
Epoch 91: train_loss=4.2589, train_acc=0.0984, val_loss=4.6184, val_acc=0.0620, lr=0.0000
Epoch 92: train_loss=4.2585, train_acc=0.1008, val_loss=4.6076, val_acc=0.0633, lr=0.0000
Epoch 93: train_loss=4.2473, train_acc=0.1018, val_loss=4.5982, val_acc=0.0645, lr=0.0000
Epoch 94: train_loss=4.2274, train_acc=0.1048, val_loss=4.5925, val_acc=0.0627, lr=0.0000
Epoch 95: train_loss=4.1959, train_acc=0.1086, val_loss=4.5907, val_acc=0.0656, lr=0.0000
Epoch 96: train_loss=4.1676, train_acc=0.1096, val_loss=4.5797, val_acc=0.0651, lr=0.0000
Epoch 97: train_loss=4.1605, train_acc=0.1119, val_loss=4.5583, val_acc=0.0678, lr=0.0000
Epoch 98: train_loss=4.1415, train_acc=0.1198, val_loss=4.5479, val_acc=0.0694, lr=0.0000
Epoch 99: train_loss=4.1185, train_acc=0.1240, val_loss=4.5473, val_acc=0.0680, lr=0.0000
Epoch 100: train_loss=4.1162, train_acc=0.1216, val_loss=4.5421, val_acc=0.0737, lr=0.0000
Training completed. Best validation accuracy: 0.0737
