Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 64,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.05,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "standard",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "global_pool": "cls+avg",
  "exp_name": "pooling_20250429_200743_poolcls+avg",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.8946, train_acc=0.0052, val_loss=5.8982, val_acc=0.0057, lr=0.0000
Epoch 2: train_loss=5.7666, train_acc=0.0035, val_loss=5.6595, val_acc=0.0055, lr=0.0000
Epoch 3: train_loss=5.5689, train_acc=0.0045, val_loss=5.4673, val_acc=0.0072, lr=0.0000
Epoch 4: train_loss=5.4192, train_acc=0.0075, val_loss=5.3481, val_acc=0.0107, lr=0.0000
Epoch 5: train_loss=5.3236, train_acc=0.0088, val_loss=5.2684, val_acc=0.0138, lr=0.0000
Epoch 6: train_loss=5.2551, train_acc=0.0133, val_loss=5.2164, val_acc=0.0142, lr=0.0000
Epoch 7: train_loss=5.1875, train_acc=0.0153, val_loss=5.1738, val_acc=0.0161, lr=0.0000
Epoch 8: train_loss=5.1426, train_acc=0.0165, val_loss=5.1323, val_acc=0.0211, lr=0.0000
Epoch 9: train_loss=5.0969, train_acc=0.0187, val_loss=5.1088, val_acc=0.0207, lr=0.0000
Epoch 10: train_loss=5.0617, train_acc=0.0153, val_loss=5.0870, val_acc=0.0207, lr=0.0000
Epoch 11: train_loss=5.0287, train_acc=0.0215, val_loss=5.0647, val_acc=0.0240, lr=0.0000
Epoch 12: train_loss=4.9896, train_acc=0.0250, val_loss=5.0277, val_acc=0.0243, lr=0.0000
Epoch 13: train_loss=4.9578, train_acc=0.0239, val_loss=5.0096, val_acc=0.0257, lr=0.0000
Epoch 14: train_loss=4.9169, train_acc=0.0307, val_loss=4.9947, val_acc=0.0254, lr=0.0000
Epoch 15: train_loss=4.8760, train_acc=0.0337, val_loss=4.9499, val_acc=0.0311, lr=0.0000
Epoch 16: train_loss=4.8273, train_acc=0.0352, val_loss=4.9011, val_acc=0.0343, lr=0.0000
Epoch 17: train_loss=4.7673, train_acc=0.0447, val_loss=4.8610, val_acc=0.0350, lr=0.0000
Epoch 18: train_loss=4.7134, train_acc=0.0450, val_loss=4.8274, val_acc=0.0450, lr=0.0000
Epoch 19: train_loss=4.6502, train_acc=0.0529, val_loss=4.7785, val_acc=0.0454, lr=0.0000
Epoch 20: train_loss=4.5770, train_acc=0.0556, val_loss=4.7071, val_acc=0.0526, lr=0.0000
Epoch 21: train_loss=4.5040, train_acc=0.0646, val_loss=4.6687, val_acc=0.0544, lr=0.0000
Epoch 22: train_loss=4.4292, train_acc=0.0684, val_loss=4.6041, val_acc=0.0594, lr=0.0000
Epoch 23: train_loss=4.3569, train_acc=0.0752, val_loss=4.5594, val_acc=0.0597, lr=0.0000
Epoch 24: train_loss=4.2905, train_acc=0.0853, val_loss=4.5123, val_acc=0.0630, lr=0.0001
Epoch 25: train_loss=4.2070, train_acc=0.0936, val_loss=4.4809, val_acc=0.0656, lr=0.0001
Epoch 26: train_loss=4.1509, train_acc=0.1028, val_loss=4.4570, val_acc=0.0701, lr=0.0001
Epoch 27: train_loss=4.1013, train_acc=0.1076, val_loss=4.4279, val_acc=0.0765, lr=0.0001
Epoch 28: train_loss=4.0477, train_acc=0.1096, val_loss=4.3641, val_acc=0.0759, lr=0.0001
Epoch 29: train_loss=3.9676, train_acc=0.1190, val_loss=4.3549, val_acc=0.0756, lr=0.0001
Epoch 30: train_loss=3.9044, train_acc=0.1305, val_loss=4.3273, val_acc=0.0804, lr=0.0001
Epoch 31: train_loss=3.8564, train_acc=0.1466, val_loss=4.2991, val_acc=0.0832, lr=0.0001
Epoch 32: train_loss=3.8061, train_acc=0.1453, val_loss=4.3048, val_acc=0.0885, lr=0.0001
Epoch 33: train_loss=3.7448, train_acc=0.1552, val_loss=4.2885, val_acc=0.0885, lr=0.0001
Epoch 34: train_loss=3.7086, train_acc=0.1510, val_loss=4.2681, val_acc=0.0889, lr=0.0001
Epoch 35: train_loss=3.6387, train_acc=0.1648, val_loss=4.2542, val_acc=0.0923, lr=0.0001
Epoch 36: train_loss=3.5904, train_acc=0.1815, val_loss=4.2394, val_acc=0.0965, lr=0.0001
Epoch 37: train_loss=3.5375, train_acc=0.1835, val_loss=4.2472, val_acc=0.0948, lr=0.0001
Epoch 38: train_loss=3.4800, train_acc=0.1925, val_loss=4.2394, val_acc=0.0961, lr=0.0001
Epoch 39: train_loss=3.4255, train_acc=0.2000, val_loss=4.2711, val_acc=0.0996, lr=0.0001
Epoch 40: train_loss=3.3698, train_acc=0.2020, val_loss=4.1999, val_acc=0.1056, lr=0.0001
Epoch 41: train_loss=3.3238, train_acc=0.2149, val_loss=4.2320, val_acc=0.1029, lr=0.0001
Epoch 42: train_loss=3.2626, train_acc=0.2302, val_loss=4.2190, val_acc=0.1039, lr=0.0001
Epoch 43: train_loss=3.1965, train_acc=0.2389, val_loss=4.2266, val_acc=0.1072, lr=0.0001
Epoch 44: train_loss=3.1563, train_acc=0.2501, val_loss=4.2169, val_acc=0.1008, lr=0.0001
Epoch 45: train_loss=3.0779, train_acc=0.2618, val_loss=4.1974, val_acc=0.1156, lr=0.0001
Epoch 46: train_loss=3.0257, train_acc=0.2691, val_loss=4.2228, val_acc=0.1094, lr=0.0001
Epoch 47: train_loss=2.9640, train_acc=0.2828, val_loss=4.2215, val_acc=0.1136, lr=0.0001
Epoch 48: train_loss=2.9057, train_acc=0.2985, val_loss=4.2732, val_acc=0.1063, lr=0.0001
Epoch 49: train_loss=2.8303, train_acc=0.3155, val_loss=4.2265, val_acc=0.1168, lr=0.0001
Epoch 50: train_loss=2.7736, train_acc=0.3173, val_loss=4.2443, val_acc=0.1162, lr=0.0001
Epoch 51: train_loss=2.7368, train_acc=0.3267, val_loss=4.2655, val_acc=0.1165, lr=0.0001
Epoch 52: train_loss=2.6629, train_acc=0.3417, val_loss=4.2824, val_acc=0.1153, lr=0.0001
Epoch 53: train_loss=2.6039, train_acc=0.3580, val_loss=4.3024, val_acc=0.1200, lr=0.0001
Epoch 54: train_loss=2.5443, train_acc=0.3735, val_loss=4.2810, val_acc=0.1174, lr=0.0001
Epoch 55: train_loss=2.4581, train_acc=0.3944, val_loss=4.3285, val_acc=0.1111, lr=0.0001
Epoch 56: train_loss=2.4392, train_acc=0.3917, val_loss=4.3523, val_acc=0.1132, lr=0.0001
Epoch 57: train_loss=2.3622, train_acc=0.4084, val_loss=4.3576, val_acc=0.1189, lr=0.0001
Epoch 58: train_loss=2.2849, train_acc=0.4298, val_loss=4.3987, val_acc=0.1146, lr=0.0001
Epoch 59: train_loss=2.2106, train_acc=0.4418, val_loss=4.3754, val_acc=0.1227, lr=0.0001
Epoch 60: train_loss=2.1554, train_acc=0.4570, val_loss=4.4016, val_acc=0.1222, lr=0.0001
Epoch 61: train_loss=2.0908, train_acc=0.4708, val_loss=4.4347, val_acc=0.1227, lr=0.0001
Epoch 62: train_loss=2.0376, train_acc=0.4843, val_loss=4.3968, val_acc=0.1219, lr=0.0001
Epoch 63: train_loss=1.9410, train_acc=0.5092, val_loss=4.4628, val_acc=0.1210, lr=0.0001
Epoch 64: train_loss=1.8944, train_acc=0.5085, val_loss=4.5230, val_acc=0.1184, lr=0.0001
Epoch 65: train_loss=1.8223, train_acc=0.5297, val_loss=4.5306, val_acc=0.1215, lr=0.0001
Epoch 66: train_loss=1.7423, train_acc=0.5534, val_loss=4.5559, val_acc=0.1155, lr=0.0001
Epoch 67: train_loss=1.6727, train_acc=0.5787, val_loss=4.5685, val_acc=0.1239, lr=0.0001
Epoch 68: train_loss=1.6050, train_acc=0.6028, val_loss=4.6222, val_acc=0.1168, lr=0.0001
Epoch 69: train_loss=1.5789, train_acc=0.5989, val_loss=4.6327, val_acc=0.1186, lr=0.0001
Epoch 70: train_loss=1.4776, train_acc=0.6230, val_loss=4.7118, val_acc=0.1189, lr=0.0001
Epoch 71: train_loss=1.4280, train_acc=0.6381, val_loss=4.7089, val_acc=0.1203, lr=0.0002
Epoch 72: train_loss=1.3871, train_acc=0.6535, val_loss=4.7883, val_acc=0.1217, lr=0.0002
Epoch 73: train_loss=1.3300, train_acc=0.6598, val_loss=4.7876, val_acc=0.1212, lr=0.0002
Epoch 74: train_loss=1.2741, train_acc=0.6777, val_loss=4.8508, val_acc=0.1167, lr=0.0002
Epoch 75: train_loss=1.2083, train_acc=0.6905, val_loss=4.8850, val_acc=0.1184, lr=0.0002
Epoch 76: train_loss=1.1641, train_acc=0.6995, val_loss=4.9322, val_acc=0.1163, lr=0.0002
Epoch 77: train_loss=1.1362, train_acc=0.7129, val_loss=4.9469, val_acc=0.1203, lr=0.0002
Epoch 78: train_loss=1.0638, train_acc=0.7334, val_loss=5.0036, val_acc=0.1177, lr=0.0002
Epoch 79: train_loss=1.0508, train_acc=0.7251, val_loss=5.0319, val_acc=0.1200, lr=0.0002
Epoch 80: train_loss=1.0063, train_acc=0.7469, val_loss=5.0728, val_acc=0.1194, lr=0.0002
Epoch 81: train_loss=0.9409, train_acc=0.7576, val_loss=5.1463, val_acc=0.1212, lr=0.0002
Epoch 82: train_loss=0.8991, train_acc=0.7641, val_loss=5.2094, val_acc=0.1148, lr=0.0002
Epoch 83: train_loss=0.8547, train_acc=0.7906, val_loss=5.2473, val_acc=0.1168, lr=0.0002
Epoch 84: train_loss=0.8273, train_acc=0.7860, val_loss=5.2881, val_acc=0.1153, lr=0.0002
Epoch 85: train_loss=0.7970, train_acc=0.7928, val_loss=5.2977, val_acc=0.1186, lr=0.0002
Epoch 86: train_loss=0.7539, train_acc=0.8050, val_loss=5.3796, val_acc=0.1144, lr=0.0002
Epoch 87: train_loss=0.7300, train_acc=0.8153, val_loss=5.4050, val_acc=0.1151, lr=0.0002
Epoch 88: train_loss=0.7126, train_acc=0.8110, val_loss=5.4459, val_acc=0.1174, lr=0.0002
Epoch 89: train_loss=0.6856, train_acc=0.8278, val_loss=5.4714, val_acc=0.1170, lr=0.0002
Epoch 90: train_loss=0.6593, train_acc=0.8335, val_loss=5.4982, val_acc=0.1130, lr=0.0002
Epoch 91: train_loss=0.6498, train_acc=0.8267, val_loss=5.5369, val_acc=0.1155, lr=0.0002
Epoch 92: train_loss=0.6231, train_acc=0.8353, val_loss=5.5685, val_acc=0.1189, lr=0.0002
Epoch 93: train_loss=0.6268, train_acc=0.8342, val_loss=5.6197, val_acc=0.1089, lr=0.0002
Epoch 94: train_loss=0.5796, train_acc=0.8475, val_loss=5.6569, val_acc=0.1212, lr=0.0002
Epoch 95: train_loss=0.5673, train_acc=0.8480, val_loss=5.6703, val_acc=0.1210, lr=0.0002
Epoch 96: train_loss=0.5536, train_acc=0.8547, val_loss=5.7585, val_acc=0.1165, lr=0.0002
Epoch 97: train_loss=0.5475, train_acc=0.8555, val_loss=5.7573, val_acc=0.1148, lr=0.0002
Epoch 98: train_loss=0.5374, train_acc=0.8579, val_loss=5.8019, val_acc=0.1189, lr=0.0002
Epoch 99: train_loss=0.4992, train_acc=0.8672, val_loss=5.8773, val_acc=0.1160, lr=0.0002
Epoch 100: train_loss=0.4978, train_acc=0.8684, val_loss=5.8575, val_acc=0.1153, lr=0.0002
Training completed. Best validation accuracy: 0.1239
