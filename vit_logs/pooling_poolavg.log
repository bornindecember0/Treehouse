Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 64,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.05,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "standard",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "global_pool": "avg",
  "exp_name": "pooling_poolavg",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.7234, train_acc=0.0035, val_loss=5.8127, val_acc=0.0022, lr=0.0000
Epoch 2: train_loss=5.6512, train_acc=0.0047, val_loss=5.6213, val_acc=0.0035, lr=0.0000
Epoch 3: train_loss=5.4682, train_acc=0.0068, val_loss=5.4312, val_acc=0.0074, lr=0.0000
Epoch 4: train_loss=5.3508, train_acc=0.0088, val_loss=5.3341, val_acc=0.0095, lr=0.0000
Epoch 5: train_loss=5.2745, train_acc=0.0102, val_loss=5.2676, val_acc=0.0121, lr=0.0000
Epoch 6: train_loss=5.2204, train_acc=0.0158, val_loss=5.2332, val_acc=0.0128, lr=0.0000
Epoch 7: train_loss=5.1718, train_acc=0.0152, val_loss=5.1915, val_acc=0.0140, lr=0.0000
Epoch 8: train_loss=5.1394, train_acc=0.0163, val_loss=5.1662, val_acc=0.0143, lr=0.0000
Epoch 9: train_loss=5.1102, train_acc=0.0187, val_loss=5.1351, val_acc=0.0204, lr=0.0000
Epoch 10: train_loss=5.0710, train_acc=0.0174, val_loss=5.1042, val_acc=0.0167, lr=0.0000
Epoch 11: train_loss=5.0355, train_acc=0.0225, val_loss=5.0890, val_acc=0.0198, lr=0.0000
Epoch 12: train_loss=5.0019, train_acc=0.0260, val_loss=5.0661, val_acc=0.0212, lr=0.0000
Epoch 13: train_loss=4.9635, train_acc=0.0267, val_loss=5.0328, val_acc=0.0195, lr=0.0000
Epoch 14: train_loss=4.9305, train_acc=0.0322, val_loss=5.0072, val_acc=0.0245, lr=0.0000
Epoch 15: train_loss=4.8875, train_acc=0.0304, val_loss=4.9698, val_acc=0.0233, lr=0.0000
Epoch 16: train_loss=4.8361, train_acc=0.0420, val_loss=4.9382, val_acc=0.0293, lr=0.0000
Epoch 17: train_loss=4.7891, train_acc=0.0429, val_loss=4.9004, val_acc=0.0323, lr=0.0000
Epoch 18: train_loss=4.7348, train_acc=0.0439, val_loss=4.8674, val_acc=0.0350, lr=0.0000
Epoch 19: train_loss=4.6882, train_acc=0.0495, val_loss=4.8267, val_acc=0.0375, lr=0.0000
Epoch 20: train_loss=4.6434, train_acc=0.0536, val_loss=4.8067, val_acc=0.0423, lr=0.0000
Epoch 21: train_loss=4.5990, train_acc=0.0584, val_loss=4.7512, val_acc=0.0431, lr=0.0000
Epoch 22: train_loss=4.5495, train_acc=0.0614, val_loss=4.7183, val_acc=0.0509, lr=0.0000
Epoch 23: train_loss=4.4959, train_acc=0.0692, val_loss=4.6753, val_acc=0.0530, lr=0.0000
Epoch 24: train_loss=4.4253, train_acc=0.0764, val_loss=4.6169, val_acc=0.0585, lr=0.0001
Epoch 25: train_loss=4.3579, train_acc=0.0848, val_loss=4.5613, val_acc=0.0680, lr=0.0001
Epoch 26: train_loss=4.2836, train_acc=0.0936, val_loss=4.5190, val_acc=0.0675, lr=0.0001
Epoch 27: train_loss=4.2189, train_acc=0.1043, val_loss=4.5025, val_acc=0.0658, lr=0.0001
Epoch 28: train_loss=4.1791, train_acc=0.1046, val_loss=4.4336, val_acc=0.0747, lr=0.0001
Epoch 29: train_loss=4.1157, train_acc=0.1133, val_loss=4.3795, val_acc=0.0768, lr=0.0001
Epoch 30: train_loss=4.0610, train_acc=0.1188, val_loss=4.3813, val_acc=0.0849, lr=0.0001
Epoch 31: train_loss=4.0089, train_acc=0.1260, val_loss=4.3429, val_acc=0.0813, lr=0.0001
Epoch 32: train_loss=3.9530, train_acc=0.1306, val_loss=4.3011, val_acc=0.0861, lr=0.0001
Epoch 33: train_loss=3.8906, train_acc=0.1438, val_loss=4.2830, val_acc=0.0897, lr=0.0001
Epoch 34: train_loss=3.8501, train_acc=0.1518, val_loss=4.2889, val_acc=0.0851, lr=0.0001
Epoch 35: train_loss=3.8036, train_acc=0.1547, val_loss=4.2523, val_acc=0.0929, lr=0.0001
Epoch 36: train_loss=3.7384, train_acc=0.1643, val_loss=4.2515, val_acc=0.0884, lr=0.0001
Epoch 37: train_loss=3.6944, train_acc=0.1685, val_loss=4.2186, val_acc=0.0939, lr=0.0001
Epoch 38: train_loss=3.6466, train_acc=0.1772, val_loss=4.2014, val_acc=0.0967, lr=0.0001
Epoch 39: train_loss=3.5955, train_acc=0.1805, val_loss=4.1700, val_acc=0.1042, lr=0.0001
Epoch 40: train_loss=3.5611, train_acc=0.1939, val_loss=4.1514, val_acc=0.1013, lr=0.0001
Epoch 41: train_loss=3.5168, train_acc=0.1952, val_loss=4.1713, val_acc=0.1061, lr=0.0001
Epoch 42: train_loss=3.4511, train_acc=0.2075, val_loss=4.1432, val_acc=0.1046, lr=0.0001
Epoch 43: train_loss=3.4087, train_acc=0.2129, val_loss=4.1111, val_acc=0.1089, lr=0.0001
Epoch 44: train_loss=3.3471, train_acc=0.2212, val_loss=4.1400, val_acc=0.1089, lr=0.0001
Epoch 45: train_loss=3.3083, train_acc=0.2296, val_loss=4.1163, val_acc=0.1129, lr=0.0001
Epoch 46: train_loss=3.2462, train_acc=0.2419, val_loss=4.0966, val_acc=0.1139, lr=0.0001
Epoch 47: train_loss=3.1980, train_acc=0.2521, val_loss=4.1159, val_acc=0.1162, lr=0.0001
Epoch 48: train_loss=3.1325, train_acc=0.2643, val_loss=4.1364, val_acc=0.1162, lr=0.0001
Epoch 49: train_loss=3.1030, train_acc=0.2656, val_loss=4.0989, val_acc=0.1177, lr=0.0001
Epoch 50: train_loss=3.0183, train_acc=0.2816, val_loss=4.0933, val_acc=0.1265, lr=0.0001
Epoch 51: train_loss=2.9622, train_acc=0.2980, val_loss=4.0893, val_acc=0.1263, lr=0.0001
Epoch 52: train_loss=2.9216, train_acc=0.3001, val_loss=4.0867, val_acc=0.1232, lr=0.0001
Epoch 53: train_loss=2.8495, train_acc=0.3192, val_loss=4.1351, val_acc=0.1182, lr=0.0001
Epoch 54: train_loss=2.8124, train_acc=0.3233, val_loss=4.1308, val_acc=0.1227, lr=0.0001
Epoch 55: train_loss=2.7465, train_acc=0.3345, val_loss=4.1278, val_acc=0.1281, lr=0.0001
Epoch 56: train_loss=2.7000, train_acc=0.3467, val_loss=4.1078, val_acc=0.1251, lr=0.0001
Epoch 57: train_loss=2.6221, train_acc=0.3604, val_loss=4.1351, val_acc=0.1248, lr=0.0001
Epoch 58: train_loss=2.5717, train_acc=0.3629, val_loss=4.1439, val_acc=0.1281, lr=0.0001
Epoch 59: train_loss=2.4925, train_acc=0.3849, val_loss=4.1557, val_acc=0.1317, lr=0.0001
Epoch 60: train_loss=2.4492, train_acc=0.3992, val_loss=4.2214, val_acc=0.1272, lr=0.0001
Epoch 61: train_loss=2.3782, train_acc=0.4191, val_loss=4.1500, val_acc=0.1344, lr=0.0001
Epoch 62: train_loss=2.2976, train_acc=0.4359, val_loss=4.1951, val_acc=0.1372, lr=0.0001
Epoch 63: train_loss=2.2685, train_acc=0.4356, val_loss=4.2645, val_acc=0.1284, lr=0.0001
Epoch 64: train_loss=2.1840, train_acc=0.4640, val_loss=4.2500, val_acc=0.1248, lr=0.0001
Epoch 65: train_loss=2.1300, train_acc=0.4695, val_loss=4.2588, val_acc=0.1334, lr=0.0001
Epoch 66: train_loss=2.0582, train_acc=0.4838, val_loss=4.3126, val_acc=0.1291, lr=0.0001
Epoch 67: train_loss=1.9991, train_acc=0.5033, val_loss=4.2795, val_acc=0.1332, lr=0.0001
Epoch 68: train_loss=1.9268, train_acc=0.5209, val_loss=4.3047, val_acc=0.1382, lr=0.0001
Epoch 69: train_loss=1.8487, train_acc=0.5394, val_loss=4.3814, val_acc=0.1315, lr=0.0001
Epoch 70: train_loss=1.7947, train_acc=0.5514, val_loss=4.3691, val_acc=0.1284, lr=0.0001
Epoch 71: train_loss=1.7342, train_acc=0.5657, val_loss=4.4358, val_acc=0.1343, lr=0.0002
Epoch 72: train_loss=1.6746, train_acc=0.5771, val_loss=4.4725, val_acc=0.1319, lr=0.0002
Epoch 73: train_loss=1.6235, train_acc=0.5918, val_loss=4.4859, val_acc=0.1305, lr=0.0002
Epoch 74: train_loss=1.5672, train_acc=0.6039, val_loss=4.4896, val_acc=0.1322, lr=0.0002
Epoch 75: train_loss=1.5155, train_acc=0.6211, val_loss=4.5953, val_acc=0.1301, lr=0.0002
Epoch 76: train_loss=1.4250, train_acc=0.6408, val_loss=4.6068, val_acc=0.1381, lr=0.0002
Epoch 77: train_loss=1.3926, train_acc=0.6483, val_loss=4.6743, val_acc=0.1300, lr=0.0002
Epoch 78: train_loss=1.3158, train_acc=0.6685, val_loss=4.7518, val_acc=0.1272, lr=0.0002
Epoch 79: train_loss=1.2702, train_acc=0.6722, val_loss=4.7609, val_acc=0.1296, lr=0.0002
Epoch 80: train_loss=1.2308, train_acc=0.6828, val_loss=4.7651, val_acc=0.1317, lr=0.0002
Epoch 81: train_loss=1.1738, train_acc=0.7084, val_loss=4.7998, val_acc=0.1346, lr=0.0002
Epoch 82: train_loss=1.1398, train_acc=0.7100, val_loss=4.8464, val_acc=0.1332, lr=0.0002
Epoch 83: train_loss=1.0673, train_acc=0.7324, val_loss=4.9191, val_acc=0.1243, lr=0.0002
Epoch 84: train_loss=1.0341, train_acc=0.7344, val_loss=4.9254, val_acc=0.1358, lr=0.0002
Epoch 85: train_loss=1.0071, train_acc=0.7431, val_loss=4.9617, val_acc=0.1293, lr=0.0002
Epoch 86: train_loss=0.9383, train_acc=0.7586, val_loss=5.0378, val_acc=0.1279, lr=0.0002
Epoch 87: train_loss=0.9157, train_acc=0.7669, val_loss=5.0756, val_acc=0.1279, lr=0.0002
Epoch 88: train_loss=0.8741, train_acc=0.7786, val_loss=5.1446, val_acc=0.1256, lr=0.0002
Epoch 89: train_loss=0.8575, train_acc=0.7831, val_loss=5.1052, val_acc=0.1260, lr=0.0002
Epoch 90: train_loss=0.8240, train_acc=0.7821, val_loss=5.1612, val_acc=0.1338, lr=0.0002
Epoch 91: train_loss=0.7904, train_acc=0.7966, val_loss=5.2392, val_acc=0.1277, lr=0.0002
Epoch 92: train_loss=0.7636, train_acc=0.8016, val_loss=5.2275, val_acc=0.1272, lr=0.0002
Epoch 93: train_loss=0.7651, train_acc=0.8023, val_loss=5.3307, val_acc=0.1260, lr=0.0002
Epoch 94: train_loss=0.7040, train_acc=0.8172, val_loss=5.3480, val_acc=0.1260, lr=0.0002
Epoch 95: train_loss=0.6766, train_acc=0.8280, val_loss=5.3371, val_acc=0.1262, lr=0.0002
Epoch 96: train_loss=0.6794, train_acc=0.8243, val_loss=5.4378, val_acc=0.1201, lr=0.0002
Epoch 97: train_loss=0.6332, train_acc=0.8335, val_loss=5.4369, val_acc=0.1288, lr=0.0002
Epoch 98: train_loss=0.6260, train_acc=0.8313, val_loss=5.5216, val_acc=0.1206, lr=0.0002
Epoch 99: train_loss=0.6145, train_acc=0.8363, val_loss=5.5890, val_acc=0.1250, lr=0.0002
Epoch 100: train_loss=0.6184, train_acc=0.8350, val_loss=5.6136, val_acc=0.1282, lr=0.0002
Training completed. Best validation accuracy: 0.1382
