Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.0001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 32,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.05,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "standard",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "use_cls_token": true,
  "global_pool": "cls",
  "exp_name": "regularization_wd0.05_drop0.1",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.7448, train_acc=0.0072, val_loss=5.7489, val_acc=0.0069, lr=0.0000
Epoch 2: train_loss=5.7391, train_acc=0.0080, val_loss=5.7361, val_acc=0.0072, lr=0.0000
Epoch 3: train_loss=5.7308, train_acc=0.0068, val_loss=5.7129, val_acc=0.0076, lr=0.0000
Epoch 4: train_loss=5.6960, train_acc=0.0062, val_loss=5.6805, val_acc=0.0076, lr=0.0000
Epoch 5: train_loss=5.6670, train_acc=0.0052, val_loss=5.6445, val_acc=0.0078, lr=0.0000
Epoch 6: train_loss=5.6411, train_acc=0.0047, val_loss=5.6060, val_acc=0.0081, lr=0.0000
Epoch 7: train_loss=5.5992, train_acc=0.0065, val_loss=5.5683, val_acc=0.0085, lr=0.0000
Epoch 8: train_loss=5.5681, train_acc=0.0073, val_loss=5.5328, val_acc=0.0074, lr=0.0000
Epoch 9: train_loss=5.5329, train_acc=0.0063, val_loss=5.5000, val_acc=0.0066, lr=0.0000
Epoch 10: train_loss=5.5057, train_acc=0.0092, val_loss=5.4695, val_acc=0.0066, lr=0.0000
Epoch 11: train_loss=5.4686, train_acc=0.0060, val_loss=5.4430, val_acc=0.0069, lr=0.0000
Epoch 12: train_loss=5.4409, train_acc=0.0065, val_loss=5.4178, val_acc=0.0079, lr=0.0000
Epoch 13: train_loss=5.4130, train_acc=0.0087, val_loss=5.3974, val_acc=0.0079, lr=0.0000
Epoch 14: train_loss=5.3933, train_acc=0.0095, val_loss=5.3768, val_acc=0.0085, lr=0.0000
Epoch 15: train_loss=5.3824, train_acc=0.0078, val_loss=5.3551, val_acc=0.0091, lr=0.0000
Epoch 16: train_loss=5.3417, train_acc=0.0113, val_loss=5.3364, val_acc=0.0110, lr=0.0000
Epoch 17: train_loss=5.3293, train_acc=0.0113, val_loss=5.3181, val_acc=0.0107, lr=0.0000
Epoch 18: train_loss=5.3182, train_acc=0.0110, val_loss=5.3028, val_acc=0.0104, lr=0.0000
Epoch 19: train_loss=5.3027, train_acc=0.0118, val_loss=5.2864, val_acc=0.0121, lr=0.0000
Epoch 20: train_loss=5.2707, train_acc=0.0127, val_loss=5.2715, val_acc=0.0123, lr=0.0000
Epoch 21: train_loss=5.2573, train_acc=0.0120, val_loss=5.2584, val_acc=0.0117, lr=0.0000
Epoch 22: train_loss=5.2503, train_acc=0.0138, val_loss=5.2434, val_acc=0.0135, lr=0.0000
Epoch 23: train_loss=5.2187, train_acc=0.0133, val_loss=5.2315, val_acc=0.0142, lr=0.0000
Epoch 24: train_loss=5.2132, train_acc=0.0155, val_loss=5.2173, val_acc=0.0131, lr=0.0000
Epoch 25: train_loss=5.2035, train_acc=0.0128, val_loss=5.2047, val_acc=0.0148, lr=0.0000
Epoch 26: train_loss=5.1815, train_acc=0.0148, val_loss=5.1952, val_acc=0.0161, lr=0.0000
Epoch 27: train_loss=5.1695, train_acc=0.0169, val_loss=5.1839, val_acc=0.0150, lr=0.0000
Epoch 28: train_loss=5.1567, train_acc=0.0177, val_loss=5.1731, val_acc=0.0135, lr=0.0000
Epoch 29: train_loss=5.1437, train_acc=0.0165, val_loss=5.1645, val_acc=0.0174, lr=0.0000
Epoch 30: train_loss=5.1303, train_acc=0.0170, val_loss=5.1515, val_acc=0.0174, lr=0.0000
Epoch 31: train_loss=5.1312, train_acc=0.0187, val_loss=5.1433, val_acc=0.0179, lr=0.0000
Epoch 32: train_loss=5.1204, train_acc=0.0165, val_loss=5.1358, val_acc=0.0197, lr=0.0000
Epoch 33: train_loss=5.1033, train_acc=0.0169, val_loss=5.1253, val_acc=0.0195, lr=0.0000
Epoch 34: train_loss=5.0847, train_acc=0.0189, val_loss=5.1178, val_acc=0.0192, lr=0.0000
Epoch 35: train_loss=5.0694, train_acc=0.0207, val_loss=5.1132, val_acc=0.0202, lr=0.0000
Epoch 36: train_loss=5.0618, train_acc=0.0205, val_loss=5.1019, val_acc=0.0219, lr=0.0000
Epoch 37: train_loss=5.0540, train_acc=0.0232, val_loss=5.0963, val_acc=0.0223, lr=0.0000
Epoch 38: train_loss=5.0367, train_acc=0.0239, val_loss=5.0890, val_acc=0.0217, lr=0.0000
Epoch 39: train_loss=5.0391, train_acc=0.0232, val_loss=5.0867, val_acc=0.0224, lr=0.0000
Epoch 40: train_loss=5.0231, train_acc=0.0249, val_loss=5.0796, val_acc=0.0216, lr=0.0000
Epoch 41: train_loss=5.0110, train_acc=0.0257, val_loss=5.0749, val_acc=0.0240, lr=0.0000
Epoch 42: train_loss=5.0056, train_acc=0.0272, val_loss=5.0663, val_acc=0.0264, lr=0.0000
Epoch 43: train_loss=4.9919, train_acc=0.0242, val_loss=5.0602, val_acc=0.0247, lr=0.0000
Epoch 44: train_loss=4.9785, train_acc=0.0252, val_loss=5.0557, val_acc=0.0240, lr=0.0000
Epoch 45: train_loss=4.9653, train_acc=0.0299, val_loss=5.0493, val_acc=0.0262, lr=0.0000
Epoch 46: train_loss=4.9615, train_acc=0.0259, val_loss=5.0419, val_acc=0.0280, lr=0.0000
Epoch 47: train_loss=4.9499, train_acc=0.0299, val_loss=5.0338, val_acc=0.0273, lr=0.0000
Epoch 48: train_loss=4.9320, train_acc=0.0297, val_loss=5.0304, val_acc=0.0268, lr=0.0000
Epoch 49: train_loss=4.9354, train_acc=0.0294, val_loss=5.0225, val_acc=0.0281, lr=0.0000
Epoch 50: train_loss=4.9134, train_acc=0.0305, val_loss=5.0238, val_acc=0.0271, lr=0.0000
Epoch 51: train_loss=4.9062, train_acc=0.0307, val_loss=5.0181, val_acc=0.0271, lr=0.0000
Epoch 52: train_loss=4.8871, train_acc=0.0337, val_loss=5.0066, val_acc=0.0307, lr=0.0000
Epoch 53: train_loss=4.8839, train_acc=0.0317, val_loss=4.9998, val_acc=0.0309, lr=0.0000
Epoch 54: train_loss=4.8736, train_acc=0.0357, val_loss=4.9932, val_acc=0.0311, lr=0.0000
Epoch 55: train_loss=4.8511, train_acc=0.0365, val_loss=4.9859, val_acc=0.0314, lr=0.0000
Epoch 56: train_loss=4.8401, train_acc=0.0369, val_loss=4.9806, val_acc=0.0338, lr=0.0000
Epoch 57: train_loss=4.8311, train_acc=0.0364, val_loss=4.9705, val_acc=0.0299, lr=0.0000
Epoch 58: train_loss=4.8222, train_acc=0.0395, val_loss=4.9653, val_acc=0.0314, lr=0.0000
Epoch 59: train_loss=4.8082, train_acc=0.0450, val_loss=4.9543, val_acc=0.0330, lr=0.0000
Epoch 60: train_loss=4.7967, train_acc=0.0417, val_loss=4.9439, val_acc=0.0337, lr=0.0000
Epoch 61: train_loss=4.7818, train_acc=0.0457, val_loss=4.9288, val_acc=0.0342, lr=0.0000
Epoch 62: train_loss=4.7701, train_acc=0.0462, val_loss=4.9220, val_acc=0.0350, lr=0.0000
Epoch 63: train_loss=4.7580, train_acc=0.0492, val_loss=4.9149, val_acc=0.0376, lr=0.0000
Epoch 64: train_loss=4.7312, train_acc=0.0532, val_loss=4.8966, val_acc=0.0356, lr=0.0000
Epoch 65: train_loss=4.7146, train_acc=0.0542, val_loss=4.8789, val_acc=0.0407, lr=0.0000
Epoch 66: train_loss=4.6957, train_acc=0.0502, val_loss=4.8637, val_acc=0.0423, lr=0.0000
Epoch 67: train_loss=4.6787, train_acc=0.0539, val_loss=4.8484, val_acc=0.0430, lr=0.0000
Epoch 68: train_loss=4.6501, train_acc=0.0574, val_loss=4.8376, val_acc=0.0444, lr=0.0000
Epoch 69: train_loss=4.6409, train_acc=0.0594, val_loss=4.8132, val_acc=0.0447, lr=0.0000
Epoch 70: train_loss=4.6253, train_acc=0.0586, val_loss=4.7963, val_acc=0.0457, lr=0.0000
Epoch 71: train_loss=4.5927, train_acc=0.0631, val_loss=4.7799, val_acc=0.0463, lr=0.0000
Epoch 72: train_loss=4.5794, train_acc=0.0639, val_loss=4.7752, val_acc=0.0450, lr=0.0000
Epoch 73: train_loss=4.5503, train_acc=0.0627, val_loss=4.7483, val_acc=0.0492, lr=0.0000
Epoch 74: train_loss=4.5400, train_acc=0.0629, val_loss=4.7400, val_acc=0.0499, lr=0.0000
Epoch 75: train_loss=4.5177, train_acc=0.0694, val_loss=4.7441, val_acc=0.0475, lr=0.0000
Epoch 76: train_loss=4.5024, train_acc=0.0701, val_loss=4.7154, val_acc=0.0499, lr=0.0000
Epoch 77: train_loss=4.4819, train_acc=0.0762, val_loss=4.6975, val_acc=0.0487, lr=0.0000
Epoch 78: train_loss=4.4538, train_acc=0.0766, val_loss=4.6839, val_acc=0.0530, lr=0.0000
Epoch 79: train_loss=4.4362, train_acc=0.0789, val_loss=4.6692, val_acc=0.0509, lr=0.0000
Epoch 80: train_loss=4.4174, train_acc=0.0812, val_loss=4.6612, val_acc=0.0535, lr=0.0000
Epoch 81: train_loss=4.4173, train_acc=0.0744, val_loss=4.6412, val_acc=0.0549, lr=0.0000
Epoch 82: train_loss=4.3791, train_acc=0.0782, val_loss=4.6375, val_acc=0.0542, lr=0.0000
Epoch 83: train_loss=4.3706, train_acc=0.0816, val_loss=4.6162, val_acc=0.0568, lr=0.0000
Epoch 84: train_loss=4.3411, train_acc=0.0809, val_loss=4.6116, val_acc=0.0578, lr=0.0000
Epoch 85: train_loss=4.3297, train_acc=0.0906, val_loss=4.6022, val_acc=0.0571, lr=0.0000
Epoch 86: train_loss=4.3122, train_acc=0.0854, val_loss=4.5919, val_acc=0.0568, lr=0.0000
Epoch 87: train_loss=4.2878, train_acc=0.0919, val_loss=4.5861, val_acc=0.0601, lr=0.0000
Epoch 88: train_loss=4.2804, train_acc=0.0953, val_loss=4.5759, val_acc=0.0557, lr=0.0000
Epoch 89: train_loss=4.2568, train_acc=0.0991, val_loss=4.5600, val_acc=0.0639, lr=0.0000
Epoch 90: train_loss=4.2420, train_acc=0.0996, val_loss=4.5431, val_acc=0.0668, lr=0.0000
Epoch 91: train_loss=4.2154, train_acc=0.1033, val_loss=4.5446, val_acc=0.0640, lr=0.0000
