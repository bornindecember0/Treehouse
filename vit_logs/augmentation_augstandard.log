Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.0001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 32,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.05,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "standard",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "use_cls_token": true,
  "global_pool": "cls",
  "exp_name": "augmentation_augstandard",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.8021, train_acc=0.0045, val_loss=5.8074, val_acc=0.0043, lr=0.0000
Epoch 2: train_loss=5.7773, train_acc=0.0047, val_loss=5.7876, val_acc=0.0041, lr=0.0000
Epoch 3: train_loss=5.7643, train_acc=0.0050, val_loss=5.7512, val_acc=0.0043, lr=0.0000
Epoch 4: train_loss=5.7257, train_acc=0.0057, val_loss=5.7070, val_acc=0.0036, lr=0.0000
Epoch 5: train_loss=5.7017, train_acc=0.0043, val_loss=5.6545, val_acc=0.0038, lr=0.0000
Epoch 6: train_loss=5.6416, train_acc=0.0052, val_loss=5.6031, val_acc=0.0041, lr=0.0000
Epoch 7: train_loss=5.5975, train_acc=0.0053, val_loss=5.5566, val_acc=0.0043, lr=0.0000
Epoch 8: train_loss=5.5503, train_acc=0.0055, val_loss=5.5105, val_acc=0.0052, lr=0.0000
Epoch 9: train_loss=5.5144, train_acc=0.0057, val_loss=5.4706, val_acc=0.0066, lr=0.0000
Epoch 10: train_loss=5.4751, train_acc=0.0063, val_loss=5.4376, val_acc=0.0074, lr=0.0000
Epoch 11: train_loss=5.4374, train_acc=0.0087, val_loss=5.4073, val_acc=0.0091, lr=0.0000
Epoch 12: train_loss=5.4118, train_acc=0.0083, val_loss=5.3809, val_acc=0.0097, lr=0.0000
Epoch 13: train_loss=5.3876, train_acc=0.0065, val_loss=5.3577, val_acc=0.0105, lr=0.0000
Epoch 14: train_loss=5.3664, train_acc=0.0082, val_loss=5.3363, val_acc=0.0116, lr=0.0000
Epoch 15: train_loss=5.3403, train_acc=0.0085, val_loss=5.3174, val_acc=0.0102, lr=0.0000
Epoch 16: train_loss=5.3259, train_acc=0.0092, val_loss=5.2997, val_acc=0.0112, lr=0.0000
Epoch 17: train_loss=5.3042, train_acc=0.0115, val_loss=5.2838, val_acc=0.0114, lr=0.0000
Epoch 18: train_loss=5.2885, train_acc=0.0118, val_loss=5.2706, val_acc=0.0114, lr=0.0000
Epoch 19: train_loss=5.2611, train_acc=0.0115, val_loss=5.2583, val_acc=0.0123, lr=0.0000
Epoch 20: train_loss=5.2630, train_acc=0.0102, val_loss=5.2449, val_acc=0.0121, lr=0.0000
Epoch 21: train_loss=5.2417, train_acc=0.0102, val_loss=5.2312, val_acc=0.0121, lr=0.0000
Epoch 22: train_loss=5.2231, train_acc=0.0120, val_loss=5.2184, val_acc=0.0133, lr=0.0000
Epoch 23: train_loss=5.2082, train_acc=0.0125, val_loss=5.2087, val_acc=0.0136, lr=0.0000
Epoch 24: train_loss=5.1941, train_acc=0.0135, val_loss=5.1965, val_acc=0.0145, lr=0.0000
Epoch 25: train_loss=5.1780, train_acc=0.0140, val_loss=5.1856, val_acc=0.0136, lr=0.0000
Epoch 26: train_loss=5.1623, train_acc=0.0145, val_loss=5.1726, val_acc=0.0143, lr=0.0000
Epoch 27: train_loss=5.1595, train_acc=0.0148, val_loss=5.1651, val_acc=0.0143, lr=0.0000
Epoch 28: train_loss=5.1464, train_acc=0.0170, val_loss=5.1536, val_acc=0.0155, lr=0.0000
Epoch 29: train_loss=5.1196, train_acc=0.0184, val_loss=5.1479, val_acc=0.0167, lr=0.0000
Epoch 30: train_loss=5.1142, train_acc=0.0212, val_loss=5.1360, val_acc=0.0162, lr=0.0000
Epoch 31: train_loss=5.0942, train_acc=0.0185, val_loss=5.1270, val_acc=0.0181, lr=0.0000
Epoch 32: train_loss=5.0885, train_acc=0.0180, val_loss=5.1196, val_acc=0.0183, lr=0.0000
Epoch 33: train_loss=5.0756, train_acc=0.0189, val_loss=5.1116, val_acc=0.0186, lr=0.0000
Epoch 34: train_loss=5.0659, train_acc=0.0184, val_loss=5.1075, val_acc=0.0185, lr=0.0000
Epoch 35: train_loss=5.0611, train_acc=0.0192, val_loss=5.0990, val_acc=0.0190, lr=0.0000
Epoch 36: train_loss=5.0466, train_acc=0.0219, val_loss=5.0902, val_acc=0.0179, lr=0.0000
Epoch 37: train_loss=5.0434, train_acc=0.0200, val_loss=5.0851, val_acc=0.0209, lr=0.0000
Epoch 38: train_loss=5.0263, train_acc=0.0249, val_loss=5.0792, val_acc=0.0212, lr=0.0000
Epoch 39: train_loss=5.0233, train_acc=0.0214, val_loss=5.0714, val_acc=0.0254, lr=0.0000
Epoch 40: train_loss=5.0074, train_acc=0.0255, val_loss=5.0635, val_acc=0.0235, lr=0.0000
Epoch 41: train_loss=4.9986, train_acc=0.0249, val_loss=5.0606, val_acc=0.0236, lr=0.0000
Epoch 42: train_loss=4.9839, train_acc=0.0247, val_loss=5.0505, val_acc=0.0255, lr=0.0000
Epoch 43: train_loss=4.9687, train_acc=0.0292, val_loss=5.0479, val_acc=0.0224, lr=0.0000
Epoch 44: train_loss=4.9609, train_acc=0.0275, val_loss=5.0400, val_acc=0.0252, lr=0.0000
Epoch 45: train_loss=4.9580, train_acc=0.0282, val_loss=5.0343, val_acc=0.0231, lr=0.0000
Epoch 46: train_loss=4.9393, train_acc=0.0305, val_loss=5.0241, val_acc=0.0264, lr=0.0000
Epoch 47: train_loss=4.9314, train_acc=0.0287, val_loss=5.0195, val_acc=0.0264, lr=0.0000
Epoch 48: train_loss=4.9190, train_acc=0.0322, val_loss=5.0107, val_acc=0.0290, lr=0.0000
Epoch 49: train_loss=4.9179, train_acc=0.0327, val_loss=5.0060, val_acc=0.0264, lr=0.0000
Epoch 50: train_loss=4.9050, train_acc=0.0377, val_loss=5.0000, val_acc=0.0257, lr=0.0000
Epoch 51: train_loss=4.8898, train_acc=0.0349, val_loss=4.9929, val_acc=0.0281, lr=0.0000
Epoch 52: train_loss=4.8827, train_acc=0.0300, val_loss=4.9899, val_acc=0.0266, lr=0.0000
Epoch 53: train_loss=4.8676, train_acc=0.0339, val_loss=4.9807, val_acc=0.0290, lr=0.0000
Epoch 54: train_loss=4.8500, train_acc=0.0410, val_loss=4.9733, val_acc=0.0288, lr=0.0000
Epoch 55: train_loss=4.8417, train_acc=0.0372, val_loss=4.9619, val_acc=0.0280, lr=0.0000
Epoch 56: train_loss=4.8335, train_acc=0.0377, val_loss=4.9581, val_acc=0.0290, lr=0.0000
Epoch 57: train_loss=4.8205, train_acc=0.0385, val_loss=4.9496, val_acc=0.0288, lr=0.0000
Epoch 58: train_loss=4.8057, train_acc=0.0394, val_loss=4.9409, val_acc=0.0302, lr=0.0000
Epoch 59: train_loss=4.7964, train_acc=0.0395, val_loss=4.9338, val_acc=0.0321, lr=0.0000
Epoch 60: train_loss=4.7808, train_acc=0.0424, val_loss=4.9284, val_acc=0.0307, lr=0.0000
Epoch 61: train_loss=4.7637, train_acc=0.0424, val_loss=4.9208, val_acc=0.0311, lr=0.0000
Epoch 62: train_loss=4.7523, train_acc=0.0439, val_loss=4.9152, val_acc=0.0314, lr=0.0000
Epoch 63: train_loss=4.7473, train_acc=0.0519, val_loss=4.9005, val_acc=0.0330, lr=0.0000
Epoch 64: train_loss=4.7346, train_acc=0.0482, val_loss=4.8980, val_acc=0.0331, lr=0.0000
Epoch 65: train_loss=4.7153, train_acc=0.0482, val_loss=4.8880, val_acc=0.0342, lr=0.0000
Epoch 66: train_loss=4.7036, train_acc=0.0462, val_loss=4.8813, val_acc=0.0340, lr=0.0000
Epoch 67: train_loss=4.7003, train_acc=0.0541, val_loss=4.8738, val_acc=0.0340, lr=0.0000
Epoch 68: train_loss=4.6709, train_acc=0.0529, val_loss=4.8608, val_acc=0.0380, lr=0.0000
Epoch 69: train_loss=4.6646, train_acc=0.0566, val_loss=4.8549, val_acc=0.0371, lr=0.0000
Epoch 70: train_loss=4.6495, train_acc=0.0532, val_loss=4.8451, val_acc=0.0362, lr=0.0000
Epoch 71: train_loss=4.6371, train_acc=0.0554, val_loss=4.8311, val_acc=0.0399, lr=0.0000
Epoch 72: train_loss=4.6189, train_acc=0.0547, val_loss=4.8261, val_acc=0.0395, lr=0.0000
Epoch 73: train_loss=4.6002, train_acc=0.0589, val_loss=4.8218, val_acc=0.0371, lr=0.0000
Epoch 74: train_loss=4.5865, train_acc=0.0621, val_loss=4.8066, val_acc=0.0406, lr=0.0000
Epoch 75: train_loss=4.5652, train_acc=0.0646, val_loss=4.7991, val_acc=0.0430, lr=0.0000
Epoch 76: train_loss=4.5358, train_acc=0.0687, val_loss=4.7946, val_acc=0.0433, lr=0.0000
Epoch 77: train_loss=4.5377, train_acc=0.0671, val_loss=4.7847, val_acc=0.0414, lr=0.0000
Epoch 78: train_loss=4.5282, train_acc=0.0686, val_loss=4.7707, val_acc=0.0431, lr=0.0000
Epoch 79: train_loss=4.5127, train_acc=0.0711, val_loss=4.7616, val_acc=0.0447, lr=0.0000
Epoch 80: train_loss=4.4812, train_acc=0.0727, val_loss=4.7566, val_acc=0.0442, lr=0.0000
Epoch 81: train_loss=4.4779, train_acc=0.0741, val_loss=4.7434, val_acc=0.0468, lr=0.0000
Epoch 82: train_loss=4.4435, train_acc=0.0772, val_loss=4.7337, val_acc=0.0482, lr=0.0000
Epoch 83: train_loss=4.4344, train_acc=0.0777, val_loss=4.7214, val_acc=0.0485, lr=0.0000
Epoch 84: train_loss=4.4122, train_acc=0.0827, val_loss=4.7158, val_acc=0.0492, lr=0.0000
Epoch 85: train_loss=4.4090, train_acc=0.0843, val_loss=4.6936, val_acc=0.0497, lr=0.0000
Epoch 86: train_loss=4.3761, train_acc=0.0853, val_loss=4.6992, val_acc=0.0502, lr=0.0000
Epoch 87: train_loss=4.3624, train_acc=0.0854, val_loss=4.6815, val_acc=0.0507, lr=0.0000
Epoch 88: train_loss=4.3541, train_acc=0.0864, val_loss=4.6642, val_acc=0.0538, lr=0.0000
Epoch 89: train_loss=4.3351, train_acc=0.0909, val_loss=4.6518, val_acc=0.0528, lr=0.0000
Epoch 90: train_loss=4.3149, train_acc=0.0938, val_loss=4.6481, val_acc=0.0554, lr=0.0000
Epoch 91: train_loss=4.2927, train_acc=0.0964, val_loss=4.6342, val_acc=0.0564, lr=0.0000
Epoch 92: train_loss=4.2825, train_acc=0.0964, val_loss=4.6282, val_acc=0.0568, lr=0.0000
Epoch 93: train_loss=4.2598, train_acc=0.0999, val_loss=4.6131, val_acc=0.0589, lr=0.0000
Epoch 94: train_loss=4.2427, train_acc=0.1001, val_loss=4.6114, val_acc=0.0552, lr=0.0000
Epoch 95: train_loss=4.2326, train_acc=0.1024, val_loss=4.6023, val_acc=0.0609, lr=0.0000
Epoch 96: train_loss=4.2110, train_acc=0.1024, val_loss=4.5912, val_acc=0.0604, lr=0.0000
Epoch 97: train_loss=4.1843, train_acc=0.1053, val_loss=4.5673, val_acc=0.0606, lr=0.0000
Epoch 98: train_loss=4.1777, train_acc=0.1064, val_loss=4.5784, val_acc=0.0628, lr=0.0000
Epoch 99: train_loss=4.1483, train_acc=0.1159, val_loss=4.5622, val_acc=0.0635, lr=0.0000
Epoch 100: train_loss=4.1236, train_acc=0.1136, val_loss=4.5551, val_acc=0.0632, lr=0.0000
Training completed. Best validation accuracy: 0.0635
