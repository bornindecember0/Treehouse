Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.0001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 32,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.05,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "minimal",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "use_cls_token": true,
  "global_pool": "cls",
  "exp_name": "augmentation_augminimal",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.8036, train_acc=0.0048, val_loss=5.8126, val_acc=0.0033, lr=0.0000
Epoch 2: train_loss=5.7952, train_acc=0.0045, val_loss=5.7953, val_acc=0.0029, lr=0.0000
Epoch 3: train_loss=5.7676, train_acc=0.0052, val_loss=5.7615, val_acc=0.0031, lr=0.0000
Epoch 4: train_loss=5.7386, train_acc=0.0052, val_loss=5.7163, val_acc=0.0033, lr=0.0000
Epoch 5: train_loss=5.6878, train_acc=0.0050, val_loss=5.6686, val_acc=0.0029, lr=0.0000
Epoch 6: train_loss=5.6517, train_acc=0.0065, val_loss=5.6167, val_acc=0.0031, lr=0.0000
Epoch 7: train_loss=5.6066, train_acc=0.0055, val_loss=5.5716, val_acc=0.0033, lr=0.0000
Epoch 8: train_loss=5.5632, train_acc=0.0063, val_loss=5.5261, val_acc=0.0048, lr=0.0000
Epoch 9: train_loss=5.5118, train_acc=0.0055, val_loss=5.4866, val_acc=0.0069, lr=0.0000
Epoch 10: train_loss=5.4790, train_acc=0.0060, val_loss=5.4510, val_acc=0.0072, lr=0.0000
Epoch 11: train_loss=5.4427, train_acc=0.0077, val_loss=5.4211, val_acc=0.0076, lr=0.0000
Epoch 12: train_loss=5.4052, train_acc=0.0093, val_loss=5.3950, val_acc=0.0090, lr=0.0000
Epoch 13: train_loss=5.3856, train_acc=0.0083, val_loss=5.3716, val_acc=0.0100, lr=0.0000
Epoch 14: train_loss=5.3562, train_acc=0.0083, val_loss=5.3497, val_acc=0.0100, lr=0.0000
Epoch 15: train_loss=5.3319, train_acc=0.0112, val_loss=5.3281, val_acc=0.0104, lr=0.0000
Epoch 16: train_loss=5.3062, train_acc=0.0113, val_loss=5.3116, val_acc=0.0110, lr=0.0000
Epoch 17: train_loss=5.2876, train_acc=0.0123, val_loss=5.2974, val_acc=0.0116, lr=0.0000
Epoch 18: train_loss=5.2692, train_acc=0.0115, val_loss=5.2835, val_acc=0.0117, lr=0.0000
Epoch 19: train_loss=5.2638, train_acc=0.0112, val_loss=5.2701, val_acc=0.0104, lr=0.0000
Epoch 20: train_loss=5.2387, train_acc=0.0130, val_loss=5.2590, val_acc=0.0112, lr=0.0000
Epoch 21: train_loss=5.2181, train_acc=0.0163, val_loss=5.2491, val_acc=0.0112, lr=0.0000
Epoch 22: train_loss=5.2168, train_acc=0.0132, val_loss=5.2368, val_acc=0.0123, lr=0.0000
Epoch 23: train_loss=5.1951, train_acc=0.0158, val_loss=5.2259, val_acc=0.0133, lr=0.0000
Epoch 24: train_loss=5.1833, train_acc=0.0152, val_loss=5.2146, val_acc=0.0128, lr=0.0000
Epoch 25: train_loss=5.1629, train_acc=0.0175, val_loss=5.2067, val_acc=0.0143, lr=0.0000
Epoch 26: train_loss=5.1545, train_acc=0.0189, val_loss=5.2005, val_acc=0.0145, lr=0.0000
Epoch 27: train_loss=5.1399, train_acc=0.0152, val_loss=5.1913, val_acc=0.0142, lr=0.0000
Epoch 28: train_loss=5.1235, train_acc=0.0177, val_loss=5.1823, val_acc=0.0157, lr=0.0000
Epoch 29: train_loss=5.1187, train_acc=0.0190, val_loss=5.1768, val_acc=0.0159, lr=0.0000
Epoch 30: train_loss=5.0965, train_acc=0.0187, val_loss=5.1686, val_acc=0.0147, lr=0.0000
Epoch 31: train_loss=5.0830, train_acc=0.0209, val_loss=5.1624, val_acc=0.0164, lr=0.0000
Epoch 32: train_loss=5.0720, train_acc=0.0215, val_loss=5.1556, val_acc=0.0173, lr=0.0000
Epoch 33: train_loss=5.0609, train_acc=0.0215, val_loss=5.1496, val_acc=0.0169, lr=0.0000
Epoch 34: train_loss=5.0499, train_acc=0.0230, val_loss=5.1439, val_acc=0.0164, lr=0.0000
Epoch 35: train_loss=5.0386, train_acc=0.0264, val_loss=5.1343, val_acc=0.0167, lr=0.0000
Epoch 36: train_loss=5.0300, train_acc=0.0235, val_loss=5.1345, val_acc=0.0159, lr=0.0000
Epoch 37: train_loss=5.0210, train_acc=0.0242, val_loss=5.1254, val_acc=0.0179, lr=0.0000
Epoch 38: train_loss=4.9948, train_acc=0.0254, val_loss=5.1227, val_acc=0.0171, lr=0.0000
Epoch 39: train_loss=4.9785, train_acc=0.0270, val_loss=5.1155, val_acc=0.0159, lr=0.0000
Epoch 40: train_loss=4.9616, train_acc=0.0282, val_loss=5.1088, val_acc=0.0166, lr=0.0000
Epoch 41: train_loss=4.9602, train_acc=0.0285, val_loss=5.1032, val_acc=0.0161, lr=0.0000
Epoch 42: train_loss=4.9509, train_acc=0.0290, val_loss=5.0985, val_acc=0.0174, lr=0.0000
Epoch 43: train_loss=4.9288, train_acc=0.0314, val_loss=5.0894, val_acc=0.0173, lr=0.0000
Epoch 44: train_loss=4.9232, train_acc=0.0295, val_loss=5.0861, val_acc=0.0200, lr=0.0000
Epoch 45: train_loss=4.9152, train_acc=0.0304, val_loss=5.0801, val_acc=0.0193, lr=0.0000
Epoch 46: train_loss=4.8904, train_acc=0.0322, val_loss=5.0743, val_acc=0.0204, lr=0.0000
Epoch 47: train_loss=4.8760, train_acc=0.0344, val_loss=5.0652, val_acc=0.0211, lr=0.0000
Epoch 48: train_loss=4.8672, train_acc=0.0360, val_loss=5.0621, val_acc=0.0200, lr=0.0000
Epoch 49: train_loss=4.8484, train_acc=0.0385, val_loss=5.0598, val_acc=0.0240, lr=0.0000
Epoch 50: train_loss=4.8401, train_acc=0.0402, val_loss=5.0556, val_acc=0.0217, lr=0.0000
Epoch 51: train_loss=4.8186, train_acc=0.0387, val_loss=5.0468, val_acc=0.0230, lr=0.0000
Epoch 52: train_loss=4.8039, train_acc=0.0435, val_loss=5.0404, val_acc=0.0231, lr=0.0000
Epoch 53: train_loss=4.7877, train_acc=0.0457, val_loss=5.0486, val_acc=0.0236, lr=0.0000
Epoch 54: train_loss=4.7778, train_acc=0.0409, val_loss=5.0350, val_acc=0.0231, lr=0.0000
Epoch 55: train_loss=4.7619, train_acc=0.0460, val_loss=5.0176, val_acc=0.0264, lr=0.0000
Epoch 56: train_loss=4.7434, train_acc=0.0442, val_loss=5.0257, val_acc=0.0238, lr=0.0000
Epoch 57: train_loss=4.7221, train_acc=0.0487, val_loss=5.0313, val_acc=0.0269, lr=0.0000
Epoch 58: train_loss=4.7146, train_acc=0.0457, val_loss=5.0059, val_acc=0.0268, lr=0.0000
Epoch 59: train_loss=4.6896, train_acc=0.0541, val_loss=4.9944, val_acc=0.0257, lr=0.0000
Epoch 60: train_loss=4.6650, train_acc=0.0527, val_loss=4.9911, val_acc=0.0249, lr=0.0000
Epoch 61: train_loss=4.6594, train_acc=0.0546, val_loss=4.9906, val_acc=0.0273, lr=0.0000
Epoch 62: train_loss=4.6381, train_acc=0.0561, val_loss=4.9903, val_acc=0.0273, lr=0.0000
Epoch 63: train_loss=4.6274, train_acc=0.0621, val_loss=4.9845, val_acc=0.0297, lr=0.0000
Epoch 64: train_loss=4.6046, train_acc=0.0597, val_loss=4.9749, val_acc=0.0287, lr=0.0000
Epoch 65: train_loss=4.5833, train_acc=0.0616, val_loss=4.9639, val_acc=0.0307, lr=0.0000
Epoch 66: train_loss=4.5686, train_acc=0.0631, val_loss=4.9552, val_acc=0.0316, lr=0.0000
Epoch 67: train_loss=4.5422, train_acc=0.0717, val_loss=4.9589, val_acc=0.0305, lr=0.0000
Epoch 68: train_loss=4.5248, train_acc=0.0654, val_loss=4.9559, val_acc=0.0316, lr=0.0000
Epoch 69: train_loss=4.5045, train_acc=0.0761, val_loss=4.9382, val_acc=0.0318, lr=0.0000
Epoch 70: train_loss=4.4831, train_acc=0.0822, val_loss=4.9361, val_acc=0.0324, lr=0.0000
Epoch 71: train_loss=4.4695, train_acc=0.0774, val_loss=4.9232, val_acc=0.0340, lr=0.0000
Epoch 72: train_loss=4.4484, train_acc=0.0809, val_loss=4.9273, val_acc=0.0337, lr=0.0000
Epoch 73: train_loss=4.4363, train_acc=0.0802, val_loss=4.9160, val_acc=0.0356, lr=0.0000
Epoch 74: train_loss=4.4081, train_acc=0.0864, val_loss=4.9069, val_acc=0.0371, lr=0.0000
Epoch 75: train_loss=4.3783, train_acc=0.0896, val_loss=4.8965, val_acc=0.0361, lr=0.0000
Epoch 76: train_loss=4.3674, train_acc=0.0911, val_loss=4.8967, val_acc=0.0354, lr=0.0000
Epoch 77: train_loss=4.3415, train_acc=0.0978, val_loss=4.8692, val_acc=0.0366, lr=0.0000
Epoch 78: train_loss=4.3210, train_acc=0.0918, val_loss=4.8760, val_acc=0.0373, lr=0.0000
Epoch 79: train_loss=4.3053, train_acc=0.0956, val_loss=4.8558, val_acc=0.0378, lr=0.0000
Epoch 80: train_loss=4.2775, train_acc=0.1038, val_loss=4.8460, val_acc=0.0395, lr=0.0000
Epoch 81: train_loss=4.2536, train_acc=0.1039, val_loss=4.8446, val_acc=0.0394, lr=0.0000
Epoch 82: train_loss=4.2301, train_acc=0.1098, val_loss=4.8233, val_acc=0.0400, lr=0.0000
Epoch 83: train_loss=4.2145, train_acc=0.1124, val_loss=4.8240, val_acc=0.0402, lr=0.0000
Epoch 84: train_loss=4.1922, train_acc=0.1131, val_loss=4.8173, val_acc=0.0406, lr=0.0000
Epoch 85: train_loss=4.1556, train_acc=0.1265, val_loss=4.8014, val_acc=0.0419, lr=0.0000
Epoch 86: train_loss=4.1284, train_acc=0.1216, val_loss=4.7993, val_acc=0.0414, lr=0.0000
Epoch 87: train_loss=4.1023, train_acc=0.1250, val_loss=4.7971, val_acc=0.0452, lr=0.0000
Epoch 88: train_loss=4.0768, train_acc=0.1386, val_loss=4.7908, val_acc=0.0454, lr=0.0000
Epoch 89: train_loss=4.0695, train_acc=0.1296, val_loss=4.7729, val_acc=0.0463, lr=0.0000
Epoch 90: train_loss=4.0295, train_acc=0.1433, val_loss=4.7724, val_acc=0.0466, lr=0.0000
Epoch 91: train_loss=4.0225, train_acc=0.1378, val_loss=4.7700, val_acc=0.0459, lr=0.0000
Epoch 92: train_loss=3.9837, train_acc=0.1441, val_loss=4.7572, val_acc=0.0490, lr=0.0000
Epoch 93: train_loss=3.9681, train_acc=0.1440, val_loss=4.7393, val_acc=0.0492, lr=0.0000
Epoch 94: train_loss=3.9327, train_acc=0.1563, val_loss=4.7534, val_acc=0.0497, lr=0.0000
Epoch 95: train_loss=3.9022, train_acc=0.1595, val_loss=4.7343, val_acc=0.0507, lr=0.0000
Epoch 96: train_loss=3.8766, train_acc=0.1625, val_loss=4.7331, val_acc=0.0516, lr=0.0000
Epoch 97: train_loss=3.8522, train_acc=0.1633, val_loss=4.7389, val_acc=0.0521, lr=0.0000
Epoch 98: train_loss=3.8405, train_acc=0.1720, val_loss=4.7236, val_acc=0.0509, lr=0.0000
Epoch 99: train_loss=3.8189, train_acc=0.1757, val_loss=4.7196, val_acc=0.0528, lr=0.0000
Epoch 100: train_loss=3.7843, train_acc=0.1818, val_loss=4.7279, val_acc=0.0513, lr=0.0000
Training completed. Best validation accuracy: 0.0528
