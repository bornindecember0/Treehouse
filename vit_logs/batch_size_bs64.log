Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.0001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 64,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.05,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "standard",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "use_cls_token": true,
  "global_pool": "cls",
  "exp_name": "batch_size_20250428_044045_bs64",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.7189, train_acc=0.0055, val_loss=5.7093, val_acc=0.0054, lr=0.0000
Epoch 2: train_loss=5.7090, train_acc=0.0058, val_loss=5.6899, val_acc=0.0055, lr=0.0000
Epoch 3: train_loss=5.6937, train_acc=0.0063, val_loss=5.6518, val_acc=0.0062, lr=0.0000
Epoch 4: train_loss=5.6596, train_acc=0.0065, val_loss=5.6078, val_acc=0.0066, lr=0.0000
Epoch 5: train_loss=5.6014, train_acc=0.0093, val_loss=5.5604, val_acc=0.0078, lr=0.0000
Epoch 6: train_loss=5.5653, train_acc=0.0045, val_loss=5.5181, val_acc=0.0085, lr=0.0000
Epoch 7: train_loss=5.5353, train_acc=0.0068, val_loss=5.4810, val_acc=0.0093, lr=0.0000
Epoch 8: train_loss=5.4756, train_acc=0.0080, val_loss=5.4472, val_acc=0.0105, lr=0.0000
Epoch 9: train_loss=5.4568, train_acc=0.0085, val_loss=5.4191, val_acc=0.0100, lr=0.0000
Epoch 10: train_loss=5.4239, train_acc=0.0075, val_loss=5.3914, val_acc=0.0105, lr=0.0000
Epoch 11: train_loss=5.4100, train_acc=0.0095, val_loss=5.3717, val_acc=0.0112, lr=0.0000
Epoch 12: train_loss=5.3738, train_acc=0.0095, val_loss=5.3495, val_acc=0.0121, lr=0.0000
Epoch 13: train_loss=5.3544, train_acc=0.0122, val_loss=5.3316, val_acc=0.0119, lr=0.0000
Epoch 14: train_loss=5.3312, train_acc=0.0095, val_loss=5.3135, val_acc=0.0116, lr=0.0000
Epoch 15: train_loss=5.3118, train_acc=0.0107, val_loss=5.2959, val_acc=0.0123, lr=0.0000
Epoch 16: train_loss=5.2984, train_acc=0.0135, val_loss=5.2795, val_acc=0.0129, lr=0.0000
Epoch 17: train_loss=5.2782, train_acc=0.0117, val_loss=5.2644, val_acc=0.0126, lr=0.0000
Epoch 18: train_loss=5.2609, train_acc=0.0120, val_loss=5.2494, val_acc=0.0136, lr=0.0000
Epoch 19: train_loss=5.2438, train_acc=0.0127, val_loss=5.2367, val_acc=0.0133, lr=0.0000
Epoch 20: train_loss=5.2299, train_acc=0.0138, val_loss=5.2232, val_acc=0.0140, lr=0.0000
Epoch 21: train_loss=5.2069, train_acc=0.0148, val_loss=5.2115, val_acc=0.0131, lr=0.0000
Epoch 22: train_loss=5.1919, train_acc=0.0130, val_loss=5.1993, val_acc=0.0150, lr=0.0000
Epoch 23: train_loss=5.1786, train_acc=0.0170, val_loss=5.1858, val_acc=0.0171, lr=0.0000
Epoch 24: train_loss=5.1641, train_acc=0.0145, val_loss=5.1767, val_acc=0.0174, lr=0.0000
Epoch 25: train_loss=5.1469, train_acc=0.0170, val_loss=5.1698, val_acc=0.0183, lr=0.0000
Epoch 26: train_loss=5.1412, train_acc=0.0189, val_loss=5.1625, val_acc=0.0188, lr=0.0000
Epoch 27: train_loss=5.1264, train_acc=0.0172, val_loss=5.1510, val_acc=0.0174, lr=0.0000
Epoch 28: train_loss=5.1105, train_acc=0.0157, val_loss=5.1406, val_acc=0.0176, lr=0.0000
Epoch 29: train_loss=5.1038, train_acc=0.0162, val_loss=5.1326, val_acc=0.0204, lr=0.0000
Epoch 30: train_loss=5.0816, train_acc=0.0209, val_loss=5.1215, val_acc=0.0205, lr=0.0000
Epoch 31: train_loss=5.0741, train_acc=0.0210, val_loss=5.1123, val_acc=0.0202, lr=0.0000
Epoch 32: train_loss=5.0552, train_acc=0.0210, val_loss=5.1007, val_acc=0.0211, lr=0.0000
Epoch 33: train_loss=5.0357, train_acc=0.0234, val_loss=5.0912, val_acc=0.0214, lr=0.0000
Epoch 34: train_loss=5.0206, train_acc=0.0215, val_loss=5.0795, val_acc=0.0198, lr=0.0000
Epoch 35: train_loss=5.0126, train_acc=0.0244, val_loss=5.0716, val_acc=0.0231, lr=0.0000
Epoch 36: train_loss=4.9879, train_acc=0.0242, val_loss=5.0553, val_acc=0.0243, lr=0.0000
Epoch 37: train_loss=4.9773, train_acc=0.0249, val_loss=5.0481, val_acc=0.0238, lr=0.0000
Epoch 38: train_loss=4.9677, train_acc=0.0262, val_loss=5.0380, val_acc=0.0243, lr=0.0000
Epoch 39: train_loss=4.9408, train_acc=0.0300, val_loss=5.0252, val_acc=0.0252, lr=0.0000
Epoch 40: train_loss=4.9256, train_acc=0.0299, val_loss=5.0103, val_acc=0.0257, lr=0.0000
Epoch 41: train_loss=4.9056, train_acc=0.0302, val_loss=5.0021, val_acc=0.0247, lr=0.0000
Epoch 42: train_loss=4.8975, train_acc=0.0319, val_loss=4.9935, val_acc=0.0264, lr=0.0000
Epoch 43: train_loss=4.8773, train_acc=0.0345, val_loss=4.9776, val_acc=0.0280, lr=0.0000
Epoch 44: train_loss=4.8611, train_acc=0.0374, val_loss=4.9667, val_acc=0.0285, lr=0.0000
Epoch 45: train_loss=4.8468, train_acc=0.0370, val_loss=4.9574, val_acc=0.0311, lr=0.0000
Epoch 46: train_loss=4.8340, train_acc=0.0410, val_loss=4.9409, val_acc=0.0347, lr=0.0000
Epoch 47: train_loss=4.8130, train_acc=0.0415, val_loss=4.9265, val_acc=0.0338, lr=0.0000
Epoch 48: train_loss=4.7999, train_acc=0.0415, val_loss=4.9129, val_acc=0.0345, lr=0.0000
Epoch 49: train_loss=4.7887, train_acc=0.0415, val_loss=4.9033, val_acc=0.0361, lr=0.0000
Epoch 50: train_loss=4.7693, train_acc=0.0420, val_loss=4.8894, val_acc=0.0369, lr=0.0000
Epoch 51: train_loss=4.7433, train_acc=0.0434, val_loss=4.8739, val_acc=0.0385, lr=0.0000
Epoch 52: train_loss=4.7230, train_acc=0.0465, val_loss=4.8630, val_acc=0.0368, lr=0.0000
Epoch 53: train_loss=4.6954, train_acc=0.0521, val_loss=4.8466, val_acc=0.0406, lr=0.0000
Epoch 54: train_loss=4.6879, train_acc=0.0521, val_loss=4.8381, val_acc=0.0435, lr=0.0000
Epoch 55: train_loss=4.6627, train_acc=0.0541, val_loss=4.8213, val_acc=0.0419, lr=0.0000
Epoch 56: train_loss=4.6429, train_acc=0.0569, val_loss=4.8093, val_acc=0.0421, lr=0.0000
Epoch 57: train_loss=4.6363, train_acc=0.0566, val_loss=4.7949, val_acc=0.0430, lr=0.0000
Epoch 58: train_loss=4.6077, train_acc=0.0634, val_loss=4.7859, val_acc=0.0469, lr=0.0000
Epoch 59: train_loss=4.6045, train_acc=0.0671, val_loss=4.7773, val_acc=0.0494, lr=0.0000
Epoch 60: train_loss=4.5757, train_acc=0.0654, val_loss=4.7702, val_acc=0.0509, lr=0.0000
Epoch 61: train_loss=4.5565, train_acc=0.0666, val_loss=4.7534, val_acc=0.0525, lr=0.0000
Epoch 62: train_loss=4.5499, train_acc=0.0676, val_loss=4.7502, val_acc=0.0513, lr=0.0000
Epoch 63: train_loss=4.5087, train_acc=0.0706, val_loss=4.7371, val_acc=0.0502, lr=0.0000
Epoch 64: train_loss=4.4885, train_acc=0.0689, val_loss=4.7184, val_acc=0.0526, lr=0.0000
Epoch 65: train_loss=4.4784, train_acc=0.0789, val_loss=4.7054, val_acc=0.0521, lr=0.0000
Epoch 66: train_loss=4.4672, train_acc=0.0724, val_loss=4.6975, val_acc=0.0533, lr=0.0000
Epoch 67: train_loss=4.4455, train_acc=0.0787, val_loss=4.6843, val_acc=0.0568, lr=0.0000
Epoch 68: train_loss=4.4252, train_acc=0.0781, val_loss=4.6670, val_acc=0.0540, lr=0.0000
Epoch 69: train_loss=4.4020, train_acc=0.0832, val_loss=4.6586, val_acc=0.0571, lr=0.0000
Epoch 70: train_loss=4.3950, train_acc=0.0844, val_loss=4.6466, val_acc=0.0590, lr=0.0000
Epoch 71: train_loss=4.3797, train_acc=0.0836, val_loss=4.6352, val_acc=0.0599, lr=0.0000
Epoch 72: train_loss=4.3571, train_acc=0.0878, val_loss=4.6283, val_acc=0.0625, lr=0.0000
Epoch 73: train_loss=4.3369, train_acc=0.0863, val_loss=4.6227, val_acc=0.0606, lr=0.0000
Epoch 74: train_loss=4.3179, train_acc=0.0903, val_loss=4.6138, val_acc=0.0620, lr=0.0000
Epoch 75: train_loss=4.2844, train_acc=0.0978, val_loss=4.5959, val_acc=0.0635, lr=0.0000
Epoch 76: train_loss=4.2784, train_acc=0.0929, val_loss=4.5918, val_acc=0.0654, lr=0.0000
Epoch 77: train_loss=4.2590, train_acc=0.1021, val_loss=4.5785, val_acc=0.0645, lr=0.0000
Epoch 78: train_loss=4.2380, train_acc=0.1039, val_loss=4.5749, val_acc=0.0664, lr=0.0000
Epoch 79: train_loss=4.2431, train_acc=0.1004, val_loss=4.5611, val_acc=0.0668, lr=0.0000
Epoch 80: train_loss=4.2036, train_acc=0.1029, val_loss=4.5465, val_acc=0.0687, lr=0.0000
Epoch 81: train_loss=4.1917, train_acc=0.1071, val_loss=4.5415, val_acc=0.0682, lr=0.0000
Epoch 82: train_loss=4.1661, train_acc=0.1108, val_loss=4.5300, val_acc=0.0675, lr=0.0000
Epoch 83: train_loss=4.1468, train_acc=0.1118, val_loss=4.5147, val_acc=0.0696, lr=0.0000
Epoch 84: train_loss=4.1361, train_acc=0.1104, val_loss=4.5091, val_acc=0.0740, lr=0.0000
Epoch 85: train_loss=4.1061, train_acc=0.1226, val_loss=4.4980, val_acc=0.0734, lr=0.0000
Epoch 86: train_loss=4.0979, train_acc=0.1256, val_loss=4.4878, val_acc=0.0739, lr=0.0000
Epoch 87: train_loss=4.0614, train_acc=0.1191, val_loss=4.4963, val_acc=0.0723, lr=0.0000
Epoch 88: train_loss=4.0533, train_acc=0.1248, val_loss=4.4827, val_acc=0.0728, lr=0.0000
Epoch 89: train_loss=4.0392, train_acc=0.1238, val_loss=4.4636, val_acc=0.0744, lr=0.0000
Epoch 90: train_loss=4.0148, train_acc=0.1345, val_loss=4.4563, val_acc=0.0749, lr=0.0000
Epoch 91: train_loss=3.9935, train_acc=0.1301, val_loss=4.4478, val_acc=0.0775, lr=0.0000
Epoch 92: train_loss=3.9646, train_acc=0.1365, val_loss=4.4478, val_acc=0.0761, lr=0.0000
Epoch 93: train_loss=3.9457, train_acc=0.1393, val_loss=4.4254, val_acc=0.0768, lr=0.0000
Epoch 94: train_loss=3.9284, train_acc=0.1416, val_loss=4.4103, val_acc=0.0777, lr=0.0000
Epoch 95: train_loss=3.9054, train_acc=0.1405, val_loss=4.4041, val_acc=0.0806, lr=0.0000
Epoch 96: train_loss=3.8977, train_acc=0.1488, val_loss=4.3917, val_acc=0.0806, lr=0.0000
Epoch 97: train_loss=3.8637, train_acc=0.1513, val_loss=4.3895, val_acc=0.0763, lr=0.0000
Epoch 98: train_loss=3.8617, train_acc=0.1542, val_loss=4.3778, val_acc=0.0809, lr=0.0000
Epoch 99: train_loss=3.8191, train_acc=0.1555, val_loss=4.3816, val_acc=0.0806, lr=0.0000
Epoch 100: train_loss=3.8181, train_acc=0.1488, val_loss=4.3778, val_acc=0.0813, lr=0.0000
Training completed. Best validation accuracy: 0.0813
