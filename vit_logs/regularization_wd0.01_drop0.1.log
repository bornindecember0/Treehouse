Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.0001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 32,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.01,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "standard",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "use_cls_token": true,
  "global_pool": "cls",
  "exp_name": "regularization_wd0.01_drop0.1",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.7966, train_acc=0.0058, val_loss=5.8261, val_acc=0.0050, lr=0.0000
Epoch 2: train_loss=5.8100, train_acc=0.0058, val_loss=5.8080, val_acc=0.0057, lr=0.0000
Epoch 3: train_loss=5.7976, train_acc=0.0062, val_loss=5.7754, val_acc=0.0057, lr=0.0000
Epoch 4: train_loss=5.7465, train_acc=0.0067, val_loss=5.7294, val_acc=0.0055, lr=0.0000
Epoch 5: train_loss=5.6965, train_acc=0.0057, val_loss=5.6809, val_acc=0.0059, lr=0.0000
Epoch 6: train_loss=5.6569, train_acc=0.0043, val_loss=5.6289, val_acc=0.0067, lr=0.0000
Epoch 7: train_loss=5.6143, train_acc=0.0050, val_loss=5.5824, val_acc=0.0066, lr=0.0000
Epoch 8: train_loss=5.5812, train_acc=0.0060, val_loss=5.5405, val_acc=0.0074, lr=0.0000
Epoch 9: train_loss=5.5507, train_acc=0.0075, val_loss=5.5011, val_acc=0.0091, lr=0.0000
Epoch 10: train_loss=5.5060, train_acc=0.0075, val_loss=5.4692, val_acc=0.0098, lr=0.0000
Epoch 11: train_loss=5.4834, train_acc=0.0087, val_loss=5.4392, val_acc=0.0097, lr=0.0000
Epoch 12: train_loss=5.4513, train_acc=0.0075, val_loss=5.4120, val_acc=0.0107, lr=0.0000
Epoch 13: train_loss=5.4156, train_acc=0.0088, val_loss=5.3871, val_acc=0.0104, lr=0.0000
Epoch 14: train_loss=5.3944, train_acc=0.0082, val_loss=5.3640, val_acc=0.0105, lr=0.0000
Epoch 15: train_loss=5.3855, train_acc=0.0062, val_loss=5.3418, val_acc=0.0098, lr=0.0000
Epoch 16: train_loss=5.3555, train_acc=0.0073, val_loss=5.3232, val_acc=0.0110, lr=0.0000
Epoch 17: train_loss=5.3356, train_acc=0.0108, val_loss=5.3066, val_acc=0.0123, lr=0.0000
Epoch 18: train_loss=5.3100, train_acc=0.0115, val_loss=5.2864, val_acc=0.0123, lr=0.0000
Epoch 19: train_loss=5.2904, train_acc=0.0133, val_loss=5.2705, val_acc=0.0109, lr=0.0000
Epoch 20: train_loss=5.2786, train_acc=0.0098, val_loss=5.2529, val_acc=0.0110, lr=0.0000
Epoch 21: train_loss=5.2620, train_acc=0.0103, val_loss=5.2405, val_acc=0.0119, lr=0.0000
Epoch 22: train_loss=5.2386, train_acc=0.0153, val_loss=5.2268, val_acc=0.0121, lr=0.0000
Epoch 23: train_loss=5.2307, train_acc=0.0138, val_loss=5.2142, val_acc=0.0126, lr=0.0000
Epoch 24: train_loss=5.2051, train_acc=0.0148, val_loss=5.2018, val_acc=0.0133, lr=0.0000
Epoch 25: train_loss=5.2013, train_acc=0.0133, val_loss=5.1925, val_acc=0.0140, lr=0.0000
Epoch 26: train_loss=5.1799, train_acc=0.0189, val_loss=5.1832, val_acc=0.0150, lr=0.0000
Epoch 27: train_loss=5.1627, train_acc=0.0167, val_loss=5.1733, val_acc=0.0169, lr=0.0000
Epoch 28: train_loss=5.1613, train_acc=0.0163, val_loss=5.1625, val_acc=0.0178, lr=0.0000
Epoch 29: train_loss=5.1372, train_acc=0.0177, val_loss=5.1536, val_acc=0.0166, lr=0.0000
Epoch 30: train_loss=5.1319, train_acc=0.0163, val_loss=5.1457, val_acc=0.0193, lr=0.0000
Epoch 31: train_loss=5.1117, train_acc=0.0207, val_loss=5.1390, val_acc=0.0209, lr=0.0000
Epoch 32: train_loss=5.1034, train_acc=0.0180, val_loss=5.1314, val_acc=0.0205, lr=0.0000
Epoch 33: train_loss=5.0995, train_acc=0.0214, val_loss=5.1250, val_acc=0.0224, lr=0.0000
Epoch 34: train_loss=5.0855, train_acc=0.0219, val_loss=5.1190, val_acc=0.0235, lr=0.0000
Epoch 35: train_loss=5.0664, train_acc=0.0215, val_loss=5.1132, val_acc=0.0228, lr=0.0000
Epoch 36: train_loss=5.0610, train_acc=0.0197, val_loss=5.1048, val_acc=0.0262, lr=0.0000
Epoch 37: train_loss=5.0484, train_acc=0.0242, val_loss=5.0978, val_acc=0.0262, lr=0.0000
Epoch 38: train_loss=5.0443, train_acc=0.0275, val_loss=5.0898, val_acc=0.0273, lr=0.0000
Epoch 39: train_loss=5.0286, train_acc=0.0257, val_loss=5.0833, val_acc=0.0250, lr=0.0000
Epoch 40: train_loss=5.0200, train_acc=0.0249, val_loss=5.0771, val_acc=0.0257, lr=0.0000
Epoch 41: train_loss=5.0044, train_acc=0.0254, val_loss=5.0687, val_acc=0.0255, lr=0.0000
Epoch 42: train_loss=4.9962, train_acc=0.0265, val_loss=5.0626, val_acc=0.0266, lr=0.0000
Epoch 43: train_loss=4.9781, train_acc=0.0274, val_loss=5.0569, val_acc=0.0274, lr=0.0000
Epoch 44: train_loss=4.9714, train_acc=0.0295, val_loss=5.0521, val_acc=0.0283, lr=0.0000
Epoch 45: train_loss=4.9566, train_acc=0.0325, val_loss=5.0425, val_acc=0.0288, lr=0.0000
Epoch 46: train_loss=4.9321, train_acc=0.0317, val_loss=5.0345, val_acc=0.0273, lr=0.0000
Epoch 47: train_loss=4.9429, train_acc=0.0290, val_loss=5.0238, val_acc=0.0269, lr=0.0000
Epoch 48: train_loss=4.9193, train_acc=0.0369, val_loss=5.0151, val_acc=0.0307, lr=0.0000
Epoch 49: train_loss=4.9045, train_acc=0.0357, val_loss=5.0076, val_acc=0.0312, lr=0.0000
Epoch 50: train_loss=4.9033, train_acc=0.0355, val_loss=5.0018, val_acc=0.0331, lr=0.0000
Epoch 51: train_loss=4.8882, train_acc=0.0374, val_loss=4.9949, val_acc=0.0328, lr=0.0000
Epoch 52: train_loss=4.8678, train_acc=0.0370, val_loss=4.9806, val_acc=0.0364, lr=0.0000
Epoch 53: train_loss=4.8611, train_acc=0.0380, val_loss=4.9713, val_acc=0.0359, lr=0.0000
Epoch 54: train_loss=4.8386, train_acc=0.0384, val_loss=4.9683, val_acc=0.0345, lr=0.0000
Epoch 55: train_loss=4.8295, train_acc=0.0390, val_loss=4.9546, val_acc=0.0369, lr=0.0000
Epoch 56: train_loss=4.8128, train_acc=0.0412, val_loss=4.9467, val_acc=0.0375, lr=0.0000
Epoch 57: train_loss=4.7936, train_acc=0.0437, val_loss=4.9372, val_acc=0.0362, lr=0.0000
Epoch 58: train_loss=4.7925, train_acc=0.0400, val_loss=4.9280, val_acc=0.0388, lr=0.0000
Epoch 59: train_loss=4.7774, train_acc=0.0434, val_loss=4.9156, val_acc=0.0388, lr=0.0000
Epoch 60: train_loss=4.7750, train_acc=0.0459, val_loss=4.9133, val_acc=0.0380, lr=0.0000
Epoch 61: train_loss=4.7483, train_acc=0.0455, val_loss=4.9036, val_acc=0.0387, lr=0.0000
Epoch 62: train_loss=4.7334, train_acc=0.0516, val_loss=4.8918, val_acc=0.0376, lr=0.0000
Epoch 63: train_loss=4.7265, train_acc=0.0459, val_loss=4.8906, val_acc=0.0388, lr=0.0000
Epoch 64: train_loss=4.7172, train_acc=0.0469, val_loss=4.8827, val_acc=0.0376, lr=0.0000
Epoch 65: train_loss=4.7000, train_acc=0.0499, val_loss=4.8693, val_acc=0.0380, lr=0.0000
Epoch 66: train_loss=4.6774, train_acc=0.0521, val_loss=4.8620, val_acc=0.0402, lr=0.0000
Epoch 67: train_loss=4.6639, train_acc=0.0506, val_loss=4.8622, val_acc=0.0404, lr=0.0000
Epoch 68: train_loss=4.6482, train_acc=0.0569, val_loss=4.8546, val_acc=0.0392, lr=0.0000
Epoch 69: train_loss=4.6279, train_acc=0.0582, val_loss=4.8439, val_acc=0.0404, lr=0.0000
Epoch 70: train_loss=4.6255, train_acc=0.0589, val_loss=4.8302, val_acc=0.0425, lr=0.0000
Epoch 71: train_loss=4.6112, train_acc=0.0552, val_loss=4.8262, val_acc=0.0426, lr=0.0000
Epoch 72: train_loss=4.5919, train_acc=0.0609, val_loss=4.8201, val_acc=0.0430, lr=0.0000
Epoch 73: train_loss=4.5774, train_acc=0.0604, val_loss=4.8129, val_acc=0.0431, lr=0.0000
Epoch 74: train_loss=4.5620, train_acc=0.0637, val_loss=4.8098, val_acc=0.0435, lr=0.0000
Epoch 75: train_loss=4.5522, train_acc=0.0626, val_loss=4.7933, val_acc=0.0459, lr=0.0000
Epoch 76: train_loss=4.5286, train_acc=0.0664, val_loss=4.7819, val_acc=0.0450, lr=0.0000
Epoch 77: train_loss=4.5195, train_acc=0.0692, val_loss=4.7715, val_acc=0.0478, lr=0.0000
Epoch 78: train_loss=4.5009, train_acc=0.0704, val_loss=4.7734, val_acc=0.0463, lr=0.0000
Epoch 79: train_loss=4.4835, train_acc=0.0746, val_loss=4.7557, val_acc=0.0480, lr=0.0000
Epoch 80: train_loss=4.4686, train_acc=0.0747, val_loss=4.7543, val_acc=0.0468, lr=0.0000
Epoch 81: train_loss=4.4366, train_acc=0.0752, val_loss=4.7350, val_acc=0.0471, lr=0.0000
Epoch 82: train_loss=4.4209, train_acc=0.0821, val_loss=4.7240, val_acc=0.0528, lr=0.0000
Epoch 83: train_loss=4.4200, train_acc=0.0799, val_loss=4.7179, val_acc=0.0506, lr=0.0000
Epoch 84: train_loss=4.4113, train_acc=0.0824, val_loss=4.6985, val_acc=0.0504, lr=0.0000
Epoch 85: train_loss=4.3794, train_acc=0.0831, val_loss=4.6819, val_acc=0.0533, lr=0.0000
Epoch 86: train_loss=4.3652, train_acc=0.0846, val_loss=4.6683, val_acc=0.0530, lr=0.0000
Epoch 87: train_loss=4.3484, train_acc=0.0863, val_loss=4.6607, val_acc=0.0545, lr=0.0000
Epoch 88: train_loss=4.3299, train_acc=0.0881, val_loss=4.6513, val_acc=0.0582, lr=0.0000
Epoch 89: train_loss=4.2961, train_acc=0.0936, val_loss=4.6378, val_acc=0.0590, lr=0.0000
Epoch 90: train_loss=4.2865, train_acc=0.1006, val_loss=4.6320, val_acc=0.0594, lr=0.0000
Epoch 91: train_loss=4.2791, train_acc=0.1004, val_loss=4.6131, val_acc=0.0620, lr=0.0000
Epoch 92: train_loss=4.2379, train_acc=0.1058, val_loss=4.6039, val_acc=0.0616, lr=0.0000
Epoch 93: train_loss=4.2391, train_acc=0.1024, val_loss=4.5908, val_acc=0.0592, lr=0.0000
Epoch 94: train_loss=4.2084, train_acc=0.1028, val_loss=4.5790, val_acc=0.0625, lr=0.0000
Epoch 95: train_loss=4.1948, train_acc=0.1083, val_loss=4.5696, val_acc=0.0652, lr=0.0000
Epoch 96: train_loss=4.1939, train_acc=0.1083, val_loss=4.5692, val_acc=0.0647, lr=0.0000
Epoch 97: train_loss=4.1678, train_acc=0.1116, val_loss=4.5518, val_acc=0.0599, lr=0.0000
Epoch 98: train_loss=4.1538, train_acc=0.1131, val_loss=4.5512, val_acc=0.0635, lr=0.0000
Epoch 99: train_loss=4.1206, train_acc=0.1235, val_loss=4.5453, val_acc=0.0671, lr=0.0000
Epoch 100: train_loss=4.1101, train_acc=0.1216, val_loss=4.5386, val_acc=0.0656, lr=0.0000
Training completed. Best validation accuracy: 0.0671
