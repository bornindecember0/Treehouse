Using device: cuda
Configuration: {
  "data_root": "cub_split",
  "num_classes": 200,
  "epochs": 100,
  "optimizer": "adamw",
  "lr": 0.001,
  "min_lr": 1e-06,
  "use_scheduler": true,
  "warmup_epochs": 5,
  "batch_size": 64,
  "val_batch_size": 64,
  "num_workers": 8,
  "weight_decay": 0.05,
  "dropout": 0.1,
  "grad_clip": 1.0,
  "img_size": 224,
  "patch_size": 16,
  "embed_dim": 192,
  "depth": 12,
  "num_heads": 3,
  "augmentation": "standard",
  "mixup_alpha": 0.0,
  "cutmix_alpha": 0.0,
  "global_pool": "cls",
  "exp_name": "pooling_20250429_200743_poolcls",
  "output_dir": "experiments"
}
Dataset sizes - Train: 5994, Val: 5794
Epoch 1: train_loss=5.7508, train_acc=0.0060, val_loss=5.7278, val_acc=0.0071, lr=0.0000
Epoch 2: train_loss=5.6867, train_acc=0.0067, val_loss=5.5776, val_acc=0.0078, lr=0.0000
Epoch 3: train_loss=5.5570, train_acc=0.0072, val_loss=5.4500, val_acc=0.0088, lr=0.0000
Epoch 4: train_loss=5.4574, train_acc=0.0077, val_loss=5.3610, val_acc=0.0100, lr=0.0000
Epoch 5: train_loss=5.3586, train_acc=0.0102, val_loss=5.2954, val_acc=0.0121, lr=0.0000
Epoch 6: train_loss=5.2975, train_acc=0.0103, val_loss=5.2459, val_acc=0.0117, lr=0.0000
Epoch 7: train_loss=5.2569, train_acc=0.0108, val_loss=5.2092, val_acc=0.0102, lr=0.0000
Epoch 8: train_loss=5.2037, train_acc=0.0123, val_loss=5.1712, val_acc=0.0107, lr=0.0000
Epoch 9: train_loss=5.1618, train_acc=0.0172, val_loss=5.1568, val_acc=0.0136, lr=0.0000
Epoch 10: train_loss=5.1281, train_acc=0.0155, val_loss=5.1226, val_acc=0.0157, lr=0.0000
Epoch 11: train_loss=5.0834, train_acc=0.0180, val_loss=5.0876, val_acc=0.0204, lr=0.0000
Epoch 12: train_loss=5.0464, train_acc=0.0192, val_loss=5.0726, val_acc=0.0205, lr=0.0000
Epoch 13: train_loss=5.0188, train_acc=0.0224, val_loss=5.0459, val_acc=0.0226, lr=0.0000
Epoch 14: train_loss=4.9727, train_acc=0.0270, val_loss=5.0138, val_acc=0.0242, lr=0.0000
Epoch 15: train_loss=4.9193, train_acc=0.0292, val_loss=4.9649, val_acc=0.0293, lr=0.0000
Epoch 16: train_loss=4.8893, train_acc=0.0357, val_loss=4.9350, val_acc=0.0281, lr=0.0000
Epoch 17: train_loss=4.8278, train_acc=0.0347, val_loss=4.9027, val_acc=0.0276, lr=0.0000
Epoch 18: train_loss=4.7817, train_acc=0.0385, val_loss=4.8452, val_acc=0.0362, lr=0.0000
Epoch 19: train_loss=4.7187, train_acc=0.0444, val_loss=4.7958, val_acc=0.0364, lr=0.0000
Epoch 20: train_loss=4.6522, train_acc=0.0537, val_loss=4.7409, val_acc=0.0435, lr=0.0000
Epoch 21: train_loss=4.5857, train_acc=0.0586, val_loss=4.6848, val_acc=0.0507, lr=0.0000
Epoch 22: train_loss=4.5111, train_acc=0.0666, val_loss=4.6275, val_acc=0.0526, lr=0.0000
Epoch 23: train_loss=4.4549, train_acc=0.0739, val_loss=4.5742, val_acc=0.0578, lr=0.0000
Epoch 24: train_loss=4.3751, train_acc=0.0806, val_loss=4.5542, val_acc=0.0589, lr=0.0001
Epoch 25: train_loss=4.3211, train_acc=0.0846, val_loss=4.5048, val_acc=0.0668, lr=0.0001
Epoch 26: train_loss=4.2623, train_acc=0.0948, val_loss=4.4651, val_acc=0.0644, lr=0.0001
Epoch 27: train_loss=4.1882, train_acc=0.0983, val_loss=4.4260, val_acc=0.0690, lr=0.0001
Epoch 28: train_loss=4.1483, train_acc=0.1046, val_loss=4.4091, val_acc=0.0777, lr=0.0001
Epoch 29: train_loss=4.0777, train_acc=0.1126, val_loss=4.3784, val_acc=0.0749, lr=0.0001
Epoch 30: train_loss=4.0351, train_acc=0.1218, val_loss=4.3327, val_acc=0.0790, lr=0.0001
Epoch 31: train_loss=3.9746, train_acc=0.1328, val_loss=4.3315, val_acc=0.0849, lr=0.0001
Epoch 32: train_loss=3.9154, train_acc=0.1433, val_loss=4.2969, val_acc=0.0915, lr=0.0001
Epoch 33: train_loss=3.8613, train_acc=0.1483, val_loss=4.2597, val_acc=0.0942, lr=0.0001
Epoch 34: train_loss=3.8058, train_acc=0.1471, val_loss=4.2669, val_acc=0.0935, lr=0.0001
Epoch 35: train_loss=3.7554, train_acc=0.1588, val_loss=4.2132, val_acc=0.0968, lr=0.0001
Epoch 36: train_loss=3.7143, train_acc=0.1687, val_loss=4.2248, val_acc=0.0908, lr=0.0001
Epoch 37: train_loss=3.6400, train_acc=0.1755, val_loss=4.2207, val_acc=0.0979, lr=0.0001
Epoch 38: train_loss=3.5975, train_acc=0.1783, val_loss=4.1942, val_acc=0.1020, lr=0.0001
Epoch 39: train_loss=3.5345, train_acc=0.1950, val_loss=4.1822, val_acc=0.0992, lr=0.0001
Epoch 40: train_loss=3.4918, train_acc=0.2047, val_loss=4.1776, val_acc=0.1029, lr=0.0001
Epoch 41: train_loss=3.4500, train_acc=0.2080, val_loss=4.1827, val_acc=0.1020, lr=0.0001
Epoch 42: train_loss=3.3528, train_acc=0.2244, val_loss=4.1539, val_acc=0.1072, lr=0.0001
Epoch 43: train_loss=3.3125, train_acc=0.2272, val_loss=4.1728, val_acc=0.1101, lr=0.0001
Epoch 44: train_loss=3.2679, train_acc=0.2437, val_loss=4.1544, val_acc=0.1075, lr=0.0001
Epoch 45: train_loss=3.1997, train_acc=0.2499, val_loss=4.1376, val_acc=0.1141, lr=0.0001
Epoch 46: train_loss=3.1379, train_acc=0.2658, val_loss=4.1301, val_acc=0.1172, lr=0.0001
Epoch 47: train_loss=3.0905, train_acc=0.2726, val_loss=4.1428, val_acc=0.1137, lr=0.0001
Epoch 48: train_loss=3.0258, train_acc=0.2916, val_loss=4.1750, val_acc=0.1093, lr=0.0001
Epoch 49: train_loss=2.9634, train_acc=0.2985, val_loss=4.1476, val_acc=0.1172, lr=0.0001
Epoch 50: train_loss=2.8891, train_acc=0.3088, val_loss=4.1551, val_acc=0.1158, lr=0.0001
Epoch 51: train_loss=2.8249, train_acc=0.3265, val_loss=4.1642, val_acc=0.1148, lr=0.0001
Epoch 52: train_loss=2.7586, train_acc=0.3347, val_loss=4.1496, val_acc=0.1191, lr=0.0001
Epoch 53: train_loss=2.7061, train_acc=0.3433, val_loss=4.1609, val_acc=0.1174, lr=0.0001
Epoch 54: train_loss=2.6272, train_acc=0.3649, val_loss=4.1617, val_acc=0.1270, lr=0.0001
Epoch 55: train_loss=2.5667, train_acc=0.3749, val_loss=4.2047, val_acc=0.1174, lr=0.0001
Epoch 56: train_loss=2.4950, train_acc=0.3934, val_loss=4.1802, val_acc=0.1217, lr=0.0001
Epoch 57: train_loss=2.4208, train_acc=0.4169, val_loss=4.2238, val_acc=0.1251, lr=0.0001
Epoch 58: train_loss=2.3875, train_acc=0.4196, val_loss=4.2029, val_acc=0.1212, lr=0.0001
Epoch 59: train_loss=2.3176, train_acc=0.4349, val_loss=4.2648, val_acc=0.1262, lr=0.0001
Epoch 60: train_loss=2.2209, train_acc=0.4641, val_loss=4.2710, val_acc=0.1201, lr=0.0001
Epoch 61: train_loss=2.1788, train_acc=0.4701, val_loss=4.2774, val_acc=0.1217, lr=0.0001
Epoch 62: train_loss=2.1007, train_acc=0.4902, val_loss=4.3316, val_acc=0.1181, lr=0.0001
Epoch 63: train_loss=2.0208, train_acc=0.5128, val_loss=4.3411, val_acc=0.1205, lr=0.0001
Epoch 64: train_loss=1.9487, train_acc=0.5172, val_loss=4.3364, val_acc=0.1215, lr=0.0001
Epoch 65: train_loss=1.8728, train_acc=0.5439, val_loss=4.3974, val_acc=0.1172, lr=0.0001
Epoch 66: train_loss=1.8275, train_acc=0.5452, val_loss=4.4321, val_acc=0.1206, lr=0.0001
Epoch 67: train_loss=1.7600, train_acc=0.5686, val_loss=4.4166, val_acc=0.1194, lr=0.0001
Epoch 68: train_loss=1.6705, train_acc=0.5949, val_loss=4.4793, val_acc=0.1229, lr=0.0001
Epoch 69: train_loss=1.5683, train_acc=0.6240, val_loss=4.4783, val_acc=0.1227, lr=0.0001
Epoch 70: train_loss=1.5374, train_acc=0.6251, val_loss=4.5164, val_acc=0.1219, lr=0.0001
Epoch 71: train_loss=1.4803, train_acc=0.6431, val_loss=4.5096, val_acc=0.1255, lr=0.0002
Epoch 72: train_loss=1.3985, train_acc=0.6583, val_loss=4.5575, val_acc=0.1274, lr=0.0002
Epoch 73: train_loss=1.3541, train_acc=0.6625, val_loss=4.6233, val_acc=0.1174, lr=0.0002
Epoch 74: train_loss=1.2850, train_acc=0.6854, val_loss=4.5874, val_acc=0.1263, lr=0.0002
Epoch 75: train_loss=1.2237, train_acc=0.7044, val_loss=4.7122, val_acc=0.1272, lr=0.0002
Epoch 76: train_loss=1.1619, train_acc=0.7162, val_loss=4.7041, val_acc=0.1210, lr=0.0002
Epoch 77: train_loss=1.1217, train_acc=0.7221, val_loss=4.7383, val_acc=0.1262, lr=0.0002
Epoch 78: train_loss=1.0557, train_acc=0.7454, val_loss=4.7724, val_acc=0.1258, lr=0.0002
Epoch 79: train_loss=1.0145, train_acc=0.7553, val_loss=4.8334, val_acc=0.1246, lr=0.0002
Epoch 80: train_loss=0.9755, train_acc=0.7676, val_loss=4.8554, val_acc=0.1232, lr=0.0002
Epoch 81: train_loss=0.9105, train_acc=0.7711, val_loss=4.9150, val_acc=0.1196, lr=0.0002
Epoch 82: train_loss=0.8998, train_acc=0.7769, val_loss=4.9263, val_acc=0.1258, lr=0.0002
Epoch 83: train_loss=0.8516, train_acc=0.7883, val_loss=4.9789, val_acc=0.1212, lr=0.0002
Epoch 84: train_loss=0.8214, train_acc=0.8028, val_loss=5.0025, val_acc=0.1300, lr=0.0002
Epoch 85: train_loss=0.7803, train_acc=0.7978, val_loss=5.0919, val_acc=0.1187, lr=0.0002
Epoch 86: train_loss=0.7550, train_acc=0.8140, val_loss=5.0857, val_acc=0.1263, lr=0.0002
Epoch 87: train_loss=0.7188, train_acc=0.8215, val_loss=5.1474, val_acc=0.1213, lr=0.0002
Epoch 88: train_loss=0.6662, train_acc=0.8335, val_loss=5.2315, val_acc=0.1179, lr=0.0002
Epoch 89: train_loss=0.6720, train_acc=0.8328, val_loss=5.2123, val_acc=0.1286, lr=0.0002
Epoch 90: train_loss=0.6223, train_acc=0.8425, val_loss=5.2349, val_acc=0.1236, lr=0.0002
Epoch 91: train_loss=0.6125, train_acc=0.8507, val_loss=5.3511, val_acc=0.1186, lr=0.0002
Epoch 92: train_loss=0.5702, train_acc=0.8519, val_loss=5.3468, val_acc=0.1167, lr=0.0002
Epoch 93: train_loss=0.5417, train_acc=0.8605, val_loss=5.3333, val_acc=0.1265, lr=0.0002
Epoch 94: train_loss=0.5646, train_acc=0.8547, val_loss=5.3825, val_acc=0.1220, lr=0.0002
Epoch 95: train_loss=0.5218, train_acc=0.8655, val_loss=5.4125, val_acc=0.1198, lr=0.0002
Epoch 96: train_loss=0.5194, train_acc=0.8637, val_loss=5.4881, val_acc=0.1224, lr=0.0002
Epoch 97: train_loss=0.4868, train_acc=0.8779, val_loss=5.5057, val_acc=0.1210, lr=0.0002
Epoch 98: train_loss=0.4787, train_acc=0.8802, val_loss=5.5969, val_acc=0.1193, lr=0.0002
Epoch 99: train_loss=0.4665, train_acc=0.8847, val_loss=5.5639, val_acc=0.1244, lr=0.0002
Epoch 100: train_loss=0.4549, train_acc=0.8810, val_loss=5.6012, val_acc=0.1232, lr=0.0002
Training completed. Best validation accuracy: 0.1300
